{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6f3c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1526724, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- Awww, c'est un bummer. Tu devrais avoir davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Est contrarié qu'il ne puisse pas mettre à jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>J'ai plongé plusieurs fois pour la balle. A ré...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tout mon corps a des démangeaisons et comme si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Non, il ne se comporte pas du tout. je suis en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  - Awww, c'est un bummer. Tu devrais avoir davi...\n",
       "1      0  Est contrarié qu'il ne puisse pas mettre à jou...\n",
       "2      0  J'ai plongé plusieurs fois pour la balle. A ré...\n",
       "3      0  Tout mon corps a des démangeaisons et comme si...\n",
       "4      0  Non, il ne se comporte pas du tout. je suis en..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#préparation du dataset\n",
    "\n",
    "import os, glob\n",
    "os.getcwd()\n",
    "import pandas as pd\n",
    "data_tweets = pd.read_csv('french_tweets.csv')\n",
    "print(data_tweets.shape)\n",
    "data_tweets.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e655e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87bf586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur : \n",
      "ngram_range (1, 1) :\n",
      "19373177\n",
      "ngram_range (1, 2) :\n",
      "38226814\n",
      "ngram_range (1, 3) :\n",
      "55631117\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur : \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(data_tweets['text']).getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7782272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur sans stopwords: \n",
      "ngram_range (1, 1) :\n",
      "19373092\n",
      "ngram_range (1, 2) :\n",
      "38226643\n",
      "ngram_range (1, 3) :\n",
      "55630860\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur sans stopwords: \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False,stop_words=[\"french\"])\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(data_tweets['text']).getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efcfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "liste_classifieurs= [\n",
    "    [\"Perceptron\", Perceptron(eta0=0.1, random_state=0)],\n",
    "    [\"MultinomialNB\", MultinomialNB()],\n",
    "    [\"Logistic Regression\", LogisticRegression()],\n",
    "    [\"linear_svc\", LinearSVC()],\n",
    "    [\"Random Forest\",RandomForestClassifier(n_estimators=250,max_depth=4, random_state=0)]\n",
    "    \n",
    "]\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0204b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultat=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6df5dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 15\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "chemin_expes = \"tweets_original.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "149568df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 1, False, False] \n",
      " 0.7151618495342977 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.71      0.74      0.73    231214\n",
      "    Négative       0.72      0.69      0.70    226804\n",
      "\n",
      "    accuracy                           0.72    458018\n",
      "   macro avg       0.72      0.71      0.71    458018\n",
      "weighted avg       0.72      0.72      0.71    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 2.087865114212036]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 1, False, False] \n",
      " 0.7656642315367518 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.80      0.78    231214\n",
      "    Négative       0.78      0.73      0.75    226804\n",
      "\n",
      "    accuracy                           0.77    458018\n",
      "   macro avg       0.77      0.77      0.77    458018\n",
      "weighted avg       0.77      0.77      0.77    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 0.6462728977203369]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 1, False, False] \n",
      " 0.79103223017436 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.78      0.79    231214\n",
      "    Négative       0.78      0.80      0.79    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 15.890681028366089]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 1, False, False] \n",
      " 0.7839298892183276 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.79      0.78      0.78    231214\n",
      "    Négative       0.78      0.79      0.78    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 19.39581847190857]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 1, False, False] \n",
      " 0.5461379247103826 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.53      0.99      0.69    231214\n",
      "    Négative       0.90      0.09      0.17    226804\n",
      "\n",
      "    accuracy                           0.55    458018\n",
      "   macro avg       0.71      0.54      0.43    458018\n",
      "weighted avg       0.71      0.55      0.43    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 211.73876976966858]\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 2, False, False] \n",
      " 0.7593216860472732 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.78      0.73      0.75    231214\n",
      "    Négative       0.74      0.79      0.77    226804\n",
      "\n",
      "    accuracy                           0.76    458018\n",
      "   macro avg       0.76      0.76      0.76    458018\n",
      "weighted avg       0.76      0.76      0.76    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 3.756520986557007]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 2, False, False] \n",
      " 0.786442890890751 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.76      0.85      0.80    231214\n",
      "    Négative       0.83      0.72      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 1.0592048168182373]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 2, False, False] \n",
      " 0.8091472387548088 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 52.432101011276245]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 2, False, False] \n",
      " 0.8032238907641185 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.81      0.79      0.80    231214\n",
      "    Négative       0.79      0.81      0.80    226804\n",
      "\n",
      "    accuracy                           0.80    458018\n",
      "   macro avg       0.80      0.80      0.80    458018\n",
      "weighted avg       0.80      0.80      0.80    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 23.25441884994507]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 2, False, False] \n",
      " 0.5105760035631788 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.95      0.01      0.02    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.73      0.51      0.35    458018\n",
      "weighted avg       0.73      0.51      0.35    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 666.3493888378143]\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 3, False, False] \n",
      " 0.7835718246881127 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.79      0.77      0.78    231214\n",
      "    Négative       0.77      0.80      0.78    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 4.7562079429626465]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 3, False, False] \n",
      " 0.7881698972529464 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.86      0.80    231214\n",
      "    Négative       0.84      0.71      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 2.171694040298462]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 3, False, False] \n",
      " 0.8071953504010759 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.79      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 168.97483897209167]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 3, False, False] \n",
      " 0.8097804016436035 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 30.720080137252808]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 3, False, False] \n",
      " 0.5098533245418302 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.97      0.01      0.02    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.74      0.51      0.35    458018\n",
      "weighted avg       0.74      0.51      0.35    458018\n",
      " \n",
      " ['Temps_execution(secondes)', 1230.8604600429535]\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase=False)\n",
    "    X = V.fit_transform(data_tweets['text'])\n",
    "    y = data_tweets['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=[\"Possitive\",\"Négative\"]\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3714fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_str = json.dumps(dic_expes, indent=4) # 缩进4字符\n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49659d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccd86246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 30\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"tweets_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_sans = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_sans = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_sans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c53c0e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 1, True, False] \n",
      " 0.7110419241165195 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.71      0.72      0.72    231214\n",
      "    Négative       0.71      0.70      0.71    226804\n",
      "\n",
      "    accuracy                           0.71    458018\n",
      "   macro avg       0.71      0.71      0.71    458018\n",
      "weighted avg       0.71      0.71      0.71    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 1.9634089469909668)\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 1, True, False] \n",
      " 0.7656707814976704 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.80      0.78    231214\n",
      "    Négative       0.78      0.73      0.75    226804\n",
      "\n",
      "    accuracy                           0.77    458018\n",
      "   macro avg       0.77      0.77      0.77    458018\n",
      "weighted avg       0.77      0.77      0.77    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 0.6620018482208252)\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 1, True, False] \n",
      " 0.7910191302525228 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.78      0.79    231214\n",
      "    Négative       0.78      0.80      0.79    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 15.141897916793823)\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 1, True, False] \n",
      " 0.78393425585894 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.79      0.78      0.78    231214\n",
      "    Négative       0.78      0.79      0.78    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 18.40116810798645)\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 1, True, False] \n",
      " 0.5619931967739259 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.54      0.98      0.69    231214\n",
      "    Négative       0.89      0.13      0.23    226804\n",
      "\n",
      "    accuracy                           0.56    458018\n",
      "   macro avg       0.71      0.56      0.46    458018\n",
      "weighted avg       0.71      0.56      0.46    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 217.5332441329956)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 2, True, False] \n",
      " 0.7581295931600941 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.77      0.76    231214\n",
      "    Négative       0.76      0.74      0.75    226804\n",
      "\n",
      "    accuracy                           0.76    458018\n",
      "   macro avg       0.76      0.76      0.76    458018\n",
      "weighted avg       0.76      0.76      0.76    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 3.8623008728027344)\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 2, True, False] \n",
      " 0.7864450742110572 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.76      0.85      0.80    231214\n",
      "    Négative       0.83      0.72      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 1.040673017501831)\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 2, True, False] \n",
      " 0.8090031396146004 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 54.619446754455566)\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 2, True, False] \n",
      " 0.8032238907641185 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.81      0.79      0.80    231214\n",
      "    Négative       0.79      0.81      0.80    226804\n",
      "\n",
      "    accuracy                           0.80    458018\n",
      "   macro avg       0.80      0.80      0.80    458018\n",
      "weighted avg       0.80      0.80      0.80    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 23.99929904937744)\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 2, True, False] \n",
      " 0.5136108187887812 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.95      0.02      0.04    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.73      0.51      0.36    458018\n",
      "weighted avg       0.73      0.51      0.36    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 668.7345070838928)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 3, True, False] \n",
      " 0.7838207232030182 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.77      0.78    231214\n",
      "    Négative       0.77      0.80      0.79    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 4.825924873352051)\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 3, True, False] \n",
      " 0.7881742638935587 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.86      0.80    231214\n",
      "    Négative       0.84      0.71      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 2.2082841396331787)\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 3, True, False] \n",
      " 0.8086014086782616 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 359.7019591331482)\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 3, True, False] \n",
      " 0.8097804016436035 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 34.96950697898865)\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 3, True, False] \n",
      " 0.508370850053928 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.96      0.01      0.01    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.74      0.50      0.34    458018\n",
      "weighted avg       0.73      0.51      0.35    458018\n",
      " \n",
      " ('Temps_execution(secondes)', 1288.0241529941559)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "en_minuscules,enlever_stopwords  = False, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase=False,stop_words=[\"french\"])\n",
    "    X = V.fit_transform(data_tweets['text'])\n",
    "    y = data_tweets['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=[\"Possitive\",\"Négative\"]\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826defba",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(dic_sans, ensure_ascii=False, indent=4) # 缩进4字符\n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1129ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e04cc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 0\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"tweets_lower.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48c24853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.7138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.72      0.71      0.71    231214\n",
      "    Négative       0.71      0.72      0.71    226804\n",
      "\n",
      "    accuracy                           0.71    458018\n",
      "   macro avg       0.71      0.71      0.71    458018\n",
      "weighted avg       0.71      0.71      0.71    458018\n",
      "\n",
      "('Temps_execution(secondes)', 1.9993071556091309)\n",
      "  MultinomialNB classifier : 0.7633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.80      0.77    231214\n",
      "    Négative       0.78      0.73      0.75    226804\n",
      "\n",
      "    accuracy                           0.76    458018\n",
      "   macro avg       0.76      0.76      0.76    458018\n",
      "weighted avg       0.76      0.76      0.76    458018\n",
      "\n",
      "('Temps_execution(secondes)', 0.6397099494934082)\n",
      "  Logistic Regression classifier : 0.7896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.78      0.79    231214\n",
      "    Négative       0.78      0.80      0.79    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      "\n",
      "('Temps_execution(secondes)', 14.806943893432617)\n",
      "  linear_svc classifier : 0.7829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.79      0.78      0.78    231214\n",
      "    Négative       0.78      0.79      0.78    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      "\n",
      "('Temps_execution(secondes)', 18.064456939697266)\n",
      "  Random Forest classifier : 0.5576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.53      0.98      0.69    231214\n",
      "    Négative       0.88      0.12      0.22    226804\n",
      "\n",
      "    accuracy                           0.56    458018\n",
      "   macro avg       0.71      0.55      0.45    458018\n",
      "weighted avg       0.71      0.56      0.46    458018\n",
      "\n",
      "('Temps_execution(secondes)', 203.4682388305664)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.7568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.77      0.74      0.75    231214\n",
      "    Négative       0.75      0.77      0.76    226804\n",
      "\n",
      "    accuracy                           0.76    458018\n",
      "   macro avg       0.76      0.76      0.76    458018\n",
      "weighted avg       0.76      0.76      0.76    458018\n",
      "\n",
      "('Temps_execution(secondes)', 3.67940092086792)\n",
      "  MultinomialNB classifier : 0.7860\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.76      0.85      0.80    231214\n",
      "    Négative       0.82      0.72      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.78    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      "\n",
      "('Temps_execution(secondes)', 1.0268261432647705)\n",
      "  Logistic Regression classifier : 0.8086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      "\n",
      "('Temps_execution(secondes)', 50.478739976882935)\n",
      "  linear_svc classifier : 0.8027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.81      0.79      0.80    231214\n",
      "    Négative       0.79      0.81      0.80    226804\n",
      "\n",
      "    accuracy                           0.80    458018\n",
      "   macro avg       0.80      0.80      0.80    458018\n",
      "weighted avg       0.80      0.80      0.80    458018\n",
      "\n",
      "('Temps_execution(secondes)', 23.904344081878662)\n",
      "  Random Forest classifier : 0.5091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.96      0.01      0.02    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.73      0.50      0.35    458018\n",
      "weighted avg       0.73      0.51      0.35    458018\n",
      "\n",
      "('Temps_execution(secondes)', 623.2433767318726)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.7815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.76      0.78    231214\n",
      "    Négative       0.76      0.81      0.79    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      "\n",
      "('Temps_execution(secondes)', 4.895364046096802)\n",
      "  MultinomialNB classifier : 0.7884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.76      0.86      0.80    231214\n",
      "    Négative       0.83      0.72      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      "\n",
      "('Temps_execution(secondes)', 2.046743869781494)\n",
      "  Logistic Regression classifier : 0.8106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      "\n",
      "('Temps_execution(secondes)', 140.1483509540558)\n",
      "  linear_svc classifier : 0.8102\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      "\n",
      "('Temps_execution(secondes)', 30.706629037857056)\n",
      "  Random Forest classifier : 0.5075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.98      0.01      0.01    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.74      0.50      0.34    458018\n",
      "weighted avg       0.74      0.51      0.34    458018\n",
      "\n",
      "('Temps_execution(secondes)', 1187.9511139392853)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase=True)\n",
    "    X = V.fit_transform(data_tweets['text'])\n",
    "    y = data_tweets['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=[\"Possitive\",\"Négative\"]\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88742024",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(dic_expes, ensure_ascii=False, indent=4) # 缩进4字符\n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71f4373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8106275299224048, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.8102105157439228, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8086166919204049, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.8027239104140012, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7896349051783991, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.7884449956115261, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.7860346099934937, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.7829233785571746, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.7815369701627447, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7632800457623936, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7567912178124004, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.713753607936806, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5575676065132812, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.5091240955595632, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.5074844220096153, \"['Random Forest', 1, 3, False, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "667b0d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 60\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"tweets_lower_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_sans = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_sans = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_sans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ce645b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.7083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.70      0.73      0.72    231214\n",
      "    Négative       0.71      0.69      0.70    226804\n",
      "\n",
      "    accuracy                           0.71    458018\n",
      "   macro avg       0.71      0.71      0.71    458018\n",
      "weighted avg       0.71      0.71      0.71    458018\n",
      "\n",
      "('Temps_execution(secondes)', 1.9757401943206787)\n",
      "  MultinomialNB classifier : 0.7633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.75      0.80      0.77    231214\n",
      "    Négative       0.78      0.73      0.75    226804\n",
      "\n",
      "    accuracy                           0.76    458018\n",
      "   macro avg       0.76      0.76      0.76    458018\n",
      "weighted avg       0.76      0.76      0.76    458018\n",
      "\n",
      "('Temps_execution(secondes)', 0.6524312496185303)\n",
      "  Logistic Regression classifier : 0.7892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.78      0.79    231214\n",
      "    Négative       0.78      0.80      0.79    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      "\n",
      "('Temps_execution(secondes)', 14.748175144195557)\n",
      "  linear_svc classifier : 0.7829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.79      0.78      0.78    231214\n",
      "    Négative       0.78      0.79      0.78    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      "\n",
      "('Temps_execution(secondes)', 18.27970290184021)\n",
      "  Random Forest classifier : 0.5511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.53      0.99      0.69    231214\n",
      "    Négative       0.92      0.10      0.18    226804\n",
      "\n",
      "    accuracy                           0.55    458018\n",
      "   macro avg       0.73      0.55      0.44    458018\n",
      "weighted avg       0.72      0.55      0.44    458018\n",
      "\n",
      "('Temps_execution(secondes)', 202.04995799064636)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.7561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.78      0.72      0.75    231214\n",
      "    Négative       0.73      0.80      0.76    226804\n",
      "\n",
      "    accuracy                           0.76    458018\n",
      "   macro avg       0.76      0.76      0.76    458018\n",
      "weighted avg       0.76      0.76      0.76    458018\n",
      "\n",
      "('Temps_execution(secondes)', 3.6307411193847656)\n",
      "  MultinomialNB classifier : 0.7860\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.76      0.85      0.80    231214\n",
      "    Négative       0.82      0.72      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.78    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      "\n",
      "('Temps_execution(secondes)', 0.9914181232452393)\n",
      "  Logistic Regression classifier : 0.8085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      "\n",
      "('Temps_execution(secondes)', 48.278650999069214)\n",
      "  linear_svc classifier : 0.8027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.81      0.79      0.80    231214\n",
      "    Négative       0.79      0.81      0.80    226804\n",
      "\n",
      "    accuracy                           0.80    458018\n",
      "   macro avg       0.80      0.80      0.80    458018\n",
      "weighted avg       0.80      0.80      0.80    458018\n",
      "\n",
      "('Temps_execution(secondes)', 22.980020761489868)\n",
      "  Random Forest classifier : 0.5119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.97      0.01      0.03    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.74      0.51      0.35    458018\n",
      "weighted avg       0.74      0.51      0.35    458018\n",
      "\n",
      "('Temps_execution(secondes)', 626.5264527797699)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.7823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.80      0.76      0.78    231214\n",
      "    Négative       0.77      0.80      0.78    226804\n",
      "\n",
      "    accuracy                           0.78    458018\n",
      "   macro avg       0.78      0.78      0.78    458018\n",
      "weighted avg       0.78      0.78      0.78    458018\n",
      "\n",
      "('Temps_execution(secondes)', 4.698688983917236)\n",
      "  MultinomialNB classifier : 0.7884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.76      0.86      0.80    231214\n",
      "    Négative       0.83      0.72      0.77    226804\n",
      "\n",
      "    accuracy                           0.79    458018\n",
      "   macro avg       0.79      0.79      0.79    458018\n",
      "weighted avg       0.79      0.79      0.79    458018\n",
      "\n",
      "('Temps_execution(secondes)', 1.9161920547485352)\n",
      "  Logistic Regression classifier : 0.8092\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      "\n",
      "('Temps_execution(secondes)', 151.29716300964355)\n",
      "  linear_svc classifier : 0.8102\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.82      0.80      0.81    231214\n",
      "    Négative       0.80      0.82      0.81    226804\n",
      "\n",
      "    accuracy                           0.81    458018\n",
      "   macro avg       0.81      0.81      0.81    458018\n",
      "weighted avg       0.81      0.81      0.81    458018\n",
      "\n",
      "('Temps_execution(secondes)', 30.3192081451416)\n",
      "  Random Forest classifier : 0.5078\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Possitive       0.51      1.00      0.67    231214\n",
      "    Négative       0.96      0.01      0.01    226804\n",
      "\n",
      "    accuracy                           0.51    458018\n",
      "   macro avg       0.73      0.50      0.34    458018\n",
      "weighted avg       0.73      0.51      0.35    458018\n",
      "\n",
      "('Temps_execution(secondes)', 1186.6252012252808)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase=True,stop_words=[\"french\"])\n",
    "    X = V.fit_transform(data_tweets['text'])\n",
    "    y = data_tweets['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=[\"Possitive\",\"Négative\"]\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f094de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(dic_expes, indent=4) \n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33d32f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8106275299224048, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.8102105157439228, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.8101908658611671, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8092171050046068, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8086166919204049, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8084704094598902, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.8027239104140012, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.8027042605312456, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7896349051783991, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.7892135243593047, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.7884471789318324, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.7884449956115261, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.7860346099934937, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.7860324266731875, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.782927745197787, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.7829233785571746, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.7822880323480736, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.7815369701627447, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7632800457623936, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.7632756791217813, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7567912178124004, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.756127488439319, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.713753607936806, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.7083236903353143, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5575676065132812, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.5510634953211446, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5119121955905663, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.5091240955595632, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.5078272032976869, \"['Random Forest', 1, 3, True, True]\"]\n",
      "[0.5074844220096153, \"['Random Forest', 1, 3, False, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85d076",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c3e05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8106275299224048, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.8106275299224048, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.8102105157439228, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.8102105157439228, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.8101908658611671, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8097804016436035, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.8092171050046068, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8091472387548088, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8090031396146004, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8086166919204049, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.8086166919204049, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8086014086782616, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.8084704094598902, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8071953504010759, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.8032238907641185, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.8027239104140012, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.8027239104140012, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.8027042605312456, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.79103223017436, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7910191302525228, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.7896349051783991, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.7896349051783991, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.7892135243593047, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.7884471789318324, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.7884449956115261, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.7884449956115261, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881742638935587, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7881698972529464, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.7864450742110572, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.786442890890751, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.7860346099934937, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.7860346099934937, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.7860324266731875, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.78393425585894, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7839298892183276, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7838207232030182, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.7835718246881127, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.782927745197787, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.782927745197787, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.7829233785571746, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.7822880323480736, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.7815369701627447, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.7815369701627447, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656707814976704, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7656642315367518, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7632800457623936, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.7632800457623936, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.7632756791217813, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7593216860472732, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7581295931600941, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.7567912178124004, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.7567912178124004, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.756127488439319, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.7151618495342977, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.713753607936806, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.713753607936806, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.7110419241165195, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.7083236903353143, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5619931967739259, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.5575676065132812, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.5575676065132812, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.5510634953211446, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5461379247103826, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5136108187887812, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.5119121955905663, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5105760035631788, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.5098533245418302, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.5091240955595632, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.5091240955595632, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.508370850053928, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.5078272032976869, \"['Random Forest', 1, 3, True, True]\"]\n",
      "[0.5074844220096153, \"['Random Forest', 1, 3, False, True]\"]\n",
      "[0.5074844220096153, \"['Random Forest', 1, 3, False, True]\"]\n"
     ]
    }
   ],
   "source": [
    "for res in sorted(Resultat,reverse=True):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0f268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "envi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "05479ac554d4c29bcd4b23e0ab8fee438ac98ff5737b36f366b93767980d4b08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
