{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57340, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore/rb ,/, as/cs an/at encouragement/n...</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "      <td>rb , cs at nn in nn nn , pps rb bez jj to vb c...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The/at Unitarian/jj clergy/nns were/bed an/at ...</td>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "      <td>at jj nns bed at jj nn in vbn nns -- cs at nn ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Ezra/np Stiles/np Gannett/np ,/, an/at honorab...</td>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "      <td>np np np , at jj nn in at nn , vbd ppl rb in a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Even/rb so/rb ,/, Gannett/np judiciously/rb ar...</td>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "      <td>rb rb , np rb vbd , at nn-tl md rb vb cs np ``...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>We/ppss today/nr are/ber not/* entitled/vbn to...</td>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "      <td>ppss nr ber * vbn to vb jj nns wps vbd np to b...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  para_id  sent_id  \\\n",
       "0     cd05        0        0   \n",
       "1     cd05        0        1   \n",
       "2     cd05        0        2   \n",
       "3     cd05        0        3   \n",
       "4     cd05        0        4   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  Furthermore/rb ,/, as/cs an/at encouragement/n...   \n",
       "1  The/at Unitarian/jj clergy/nns were/bed an/at ...   \n",
       "2  Ezra/np Stiles/np Gannett/np ,/, an/at honorab...   \n",
       "3  Even/rb so/rb ,/, Gannett/np judiciously/rb ar...   \n",
       "4  We/ppss today/nr are/ber not/* entitled/vbn to...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  Furthermore , as an encouragement to revisioni...   \n",
       "1  The Unitarian clergy were an exclusive club of...   \n",
       "2  Ezra Stiles Gannett , an honorable representat...   \n",
       "3  Even so , Gannett judiciously argued , the Ass...   \n",
       "4  We today are not entitled to excoriate honest ...   \n",
       "\n",
       "                                       tokenized_pos     label  \n",
       "0  rb , cs at nn in nn nn , pps rb bez jj to vb c...  religion  \n",
       "1  at jj nns bed at jj nn in vbn nns -- cs at nn ...  religion  \n",
       "2  np np np , at jj nn in at nn , vbd ppl rb in a...  religion  \n",
       "3  rb rb , np rb vbd , at nn-tl md rb vb cs np ``...  religion  \n",
       "4  ppss nr ber * vbn to vb jj nns wps vbd np to b...  religion  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob\n",
    "os.getcwd()\n",
    "import pandas as pd\n",
    "brown = pd.read_csv('archive-2/brown.csv')\n",
    "print(brown.shape)\n",
    "brown.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le somme des mots dans l'ensemble:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1161169"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=0\n",
    "list_word=[]\n",
    "for i in brown['tokenized_text']:\n",
    "    word=i.split()\n",
    "    for w in word:\n",
    "        words+=1\n",
    "        list_word.append(w)\n",
    "print(\"le somme des mots dans l'ensemble:\")\n",
    "len(set(list_word))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catégorie</th>\n",
       "      <th>Nombre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>religion</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lore</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learned</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mystery</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adventure</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>government</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>editorial</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>humor</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hobbies</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>romance</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>news</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fiction</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>reviews</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>science_fiction</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Catégorie  Nombre\n",
       "0          religion     354\n",
       "1              lore     224\n",
       "2           learned     263\n",
       "3           mystery      25\n",
       "4         adventure      89\n",
       "5        government     223\n",
       "6         editorial     314\n",
       "7             humor     250\n",
       "8           hobbies     429\n",
       "9    belles_lettres     499\n",
       "10          romance     320\n",
       "11             news     300\n",
       "12          fiction      82\n",
       "13          reviews     285\n",
       "14  science_fiction     112"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = zip(brown['label'], brown['raw_text'])\n",
    "\n",
    "r=list(result)\n",
    "dic={}\n",
    "for i in r:\n",
    "    dic[i[0]]=i[1]\n",
    "\n",
    "\n",
    "data = {'Catégorie':dic.keys(), 'Nombre':([len(contenu) for k,contenu in dic.items()])}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur : \n",
      "ngram_range (1, 1) :\n",
      "1425024\n",
      "ngram_range (1, 2) :\n",
      "3257472\n",
      "ngram_range (1, 3) :\n",
      "5134617\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur : \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N))\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(brown['raw_text']).getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur sans stopwords: \n",
      "ngram_range (1, 1) :\n",
      "991438\n",
      "ngram_range (1, 2) :\n",
      "2227475\n",
      "ngram_range (1, 3) :\n",
      "3435181\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur sans stopwords: \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False,stop_words='english')\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(brown['raw_text']).getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultat=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "liste_classifieurs= [\n",
    "    [\"Perceptron\", Perceptron(eta0=0.1, random_state=0)],\n",
    "    [\"MultinomialNB\", MultinomialNB()],\n",
    "    [\"Logistic Regression\", LogisticRegression()],\n",
    "    [\"linear_svc\", LinearSVC()],\n",
    "    [\"Random Forest\",RandomForestClassifier(n_estimators=250,max_depth=4, random_state=0)],\n",
    "    [\"Decision Tree\", DecisionTreeClassifier(criterion='gini',max_depth=4,splitter='random',min_samples_split=5)],\n",
    "    [\"Perceptron multicouche\",MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(30,20), random_state=1)]\n",
    "\n",
    "]\n",
    "## Pour éviter les warnings (merci julien):\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "chemin_expes = \"Pos_original.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification utilisant les textes avec POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 1, False, False] \n",
      " 0.4644808743169399 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.47      0.45      1449\n",
      "        mystery       0.39      0.57      0.46      2121\n",
      "     government       0.29      0.32      0.30       904\n",
      "        hobbies       0.44      0.35      0.39      1268\n",
      "           news       0.60      0.54      0.57       890\n",
      "        fiction       0.52      0.58      0.55      1281\n",
      "        reviews       0.35      0.20      0.26       314\n",
      "           lore       0.70      0.62      0.66      2288\n",
      "      editorial       0.50      0.31      0.39      1422\n",
      " belles_lettres       0.44      0.43      0.44      1186\n",
      "      adventure       0.49      0.51      0.50      1387\n",
      "        romance       0.32      0.42      0.37       515\n",
      "       religion       0.46      0.31      0.37       550\n",
      "science_fiction       0.42      0.37      0.40      1361\n",
      "          humor       0.27      0.35      0.30       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.44      0.42      0.43     17202\n",
      "   weighted avg       0.47      0.46      0.46     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.7794699668884277]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 1, False, False] \n",
      " 0.3057202650854552 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.55      0.26      0.36      1449\n",
      "        mystery       0.19      0.83      0.31      2121\n",
      "     government       1.00      0.00      0.00       904\n",
      "        hobbies       0.71      0.12      0.21      1268\n",
      "           news       0.96      0.05      0.10       890\n",
      "        fiction       0.89      0.11      0.19      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.36      0.84      0.50      2288\n",
      "      editorial       0.74      0.05      0.10      1422\n",
      " belles_lettres       0.84      0.11      0.20      1186\n",
      "      adventure       0.67      0.24      0.35      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.42      0.24      0.30      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.31     17202\n",
      "      macro avg       0.49      0.19      0.17     17202\n",
      "   weighted avg       0.54      0.31      0.24     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.3714907169342041]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 1, False, False] \n",
      " 0.46918962911289386 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.48      0.46      1449\n",
      "        mystery       0.38      0.62      0.47      2121\n",
      "     government       0.51      0.17      0.26       904\n",
      "        hobbies       0.43      0.37      0.40      1268\n",
      "           news       0.62      0.55      0.58       890\n",
      "        fiction       0.57      0.55      0.56      1281\n",
      "        reviews       0.88      0.05      0.09       314\n",
      "           lore       0.53      0.76      0.62      2288\n",
      "      editorial       0.45      0.32      0.37      1422\n",
      " belles_lettres       0.44      0.40      0.42      1186\n",
      "      adventure       0.52      0.53      0.53      1387\n",
      "        romance       0.69      0.17      0.27       515\n",
      "       religion       0.65      0.14      0.23       550\n",
      "science_fiction       0.40      0.47      0.43      1361\n",
      "          humor       0.86      0.05      0.09       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.56      0.38      0.39     17202\n",
      "   weighted avg       0.50      0.47      0.45     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 8.447808980941772]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 1, False, False] \n",
      " 0.535344727357284 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.51      0.50      1449\n",
      "        mystery       0.48      0.59      0.53      2121\n",
      "     government       0.47      0.31      0.37       904\n",
      "        hobbies       0.47      0.44      0.45      1268\n",
      "           news       0.62      0.67      0.65       890\n",
      "        fiction       0.62      0.63      0.63      1281\n",
      "        reviews       0.60      0.25      0.35       314\n",
      "           lore       0.66      0.75      0.70      2288\n",
      "      editorial       0.48      0.45      0.47      1422\n",
      " belles_lettres       0.47      0.49      0.48      1186\n",
      "      adventure       0.58      0.56      0.57      1387\n",
      "        romance       0.55      0.43      0.49       515\n",
      "       religion       0.59      0.36      0.45       550\n",
      "science_fiction       0.45      0.50      0.48      1361\n",
      "          humor       0.61      0.32      0.42       266\n",
      "\n",
      "       accuracy                           0.54     17202\n",
      "      macro avg       0.54      0.48      0.50     17202\n",
      "   weighted avg       0.54      0.54      0.53     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 2.3982629776000977]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 1, False, False] \n",
      " 0.1417277060806883 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.22      0.08      0.11      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      0.99      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.02      0.07      0.02     17202\n",
      "   weighted avg       0.05      0.14      0.05     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 6.445034027099609]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 1, False, False] \n",
      " 0.18218811766073711 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.18      0.33      0.23      1449\n",
      "        mystery       0.19      0.13      0.16      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.23      0.10      0.14       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.21      0.63      0.31      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.20      0.23      0.22      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.12      0.39      0.19      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.14      0.12      0.08     17202\n",
      "   weighted avg       0.14      0.18      0.12     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.0785937309265137]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 1, False, False] \n",
      " 0.41721892803162425 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.44      0.40      0.42      1449\n",
      "        mystery       0.45      0.47      0.46      2121\n",
      "     government       0.23      0.20      0.21       904\n",
      "        hobbies       0.31      0.33      0.32      1268\n",
      "           news       0.44      0.53      0.48       890\n",
      "        fiction       0.58      0.51      0.54      1281\n",
      "        reviews       0.04      0.01      0.02       314\n",
      "           lore       0.64      0.64      0.64      2288\n",
      "      editorial       0.27      0.35      0.30      1422\n",
      " belles_lettres       0.35      0.38      0.37      1186\n",
      "      adventure       0.50      0.51      0.50      1387\n",
      "        romance       0.32      0.21      0.26       515\n",
      "       religion       0.12      0.06      0.08       550\n",
      "science_fiction       0.35      0.42      0.38      1361\n",
      "          humor       0.13      0.04      0.06       266\n",
      "\n",
      "       accuracy                           0.42     17202\n",
      "      macro avg       0.34      0.34      0.34     17202\n",
      "   weighted avg       0.41      0.42      0.41     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 31.6541531085968]\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 2, False, False] \n",
      " 0.4856412045111034 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.44      0.44      1449\n",
      "        mystery       0.44      0.56      0.50      2121\n",
      "     government       0.34      0.32      0.33       904\n",
      "        hobbies       0.45      0.35      0.40      1268\n",
      "           news       0.60      0.63      0.61       890\n",
      "        fiction       0.57      0.57      0.57      1281\n",
      "        reviews       0.26      0.26      0.26       314\n",
      "           lore       0.68      0.68      0.68      2288\n",
      "      editorial       0.47      0.39      0.42      1422\n",
      " belles_lettres       0.45      0.42      0.44      1186\n",
      "      adventure       0.52      0.57      0.54      1387\n",
      "        romance       0.40      0.43      0.41       515\n",
      "       religion       0.36      0.37      0.36       550\n",
      "science_fiction       0.44      0.38      0.41      1361\n",
      "          humor       0.30      0.30      0.30       266\n",
      "\n",
      "       accuracy                           0.49     17202\n",
      "      macro avg       0.45      0.44      0.44     17202\n",
      "   weighted avg       0.49      0.49      0.48     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.2434988021850586]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 2, False, False] \n",
      " 0.28618765259853507 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.53      0.21      0.30      1449\n",
      "        mystery       0.18      0.82      0.30      2121\n",
      "     government       1.00      0.00      0.01       904\n",
      "        hobbies       0.69      0.09      0.16      1268\n",
      "           news       0.85      0.07      0.13       890\n",
      "        fiction       0.90      0.09      0.16      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.35      0.84      0.49      2288\n",
      "      editorial       0.76      0.03      0.06      1422\n",
      " belles_lettres       0.73      0.08      0.14      1186\n",
      "      adventure       0.69      0.19      0.30      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.39      0.21      0.27      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.29     17202\n",
      "      macro avg       0.47      0.17      0.15     17202\n",
      "   weighted avg       0.53      0.29      0.22     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.5085270404815674]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 2, False, False] \n",
      " 0.4629694221602139 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.48      0.45      1449\n",
      "        mystery       0.37      0.62      0.46      2121\n",
      "     government       0.54      0.15      0.24       904\n",
      "        hobbies       0.42      0.36      0.39      1268\n",
      "           news       0.70      0.52      0.59       890\n",
      "        fiction       0.56      0.54      0.55      1281\n",
      "        reviews       1.00      0.02      0.04       314\n",
      "           lore       0.51      0.77      0.62      2288\n",
      "      editorial       0.44      0.31      0.37      1422\n",
      " belles_lettres       0.45      0.39      0.42      1186\n",
      "      adventure       0.52      0.54      0.53      1387\n",
      "        romance       0.78      0.17      0.27       515\n",
      "       religion       0.77      0.10      0.18       550\n",
      "science_fiction       0.39      0.46      0.42      1361\n",
      "          humor       0.88      0.03      0.05       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.58      0.36      0.37     17202\n",
      "   weighted avg       0.51      0.46      0.44     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 36.449394941329956]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 2, False, False] \n",
      " 0.5413905359841878 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.50      0.48      1449\n",
      "        mystery       0.49      0.61      0.54      2121\n",
      "     government       0.50      0.31      0.38       904\n",
      "        hobbies       0.46      0.44      0.45      1268\n",
      "           news       0.68      0.66      0.67       890\n",
      "        fiction       0.62      0.64      0.63      1281\n",
      "        reviews       0.64      0.25      0.36       314\n",
      "           lore       0.65      0.76      0.70      2288\n",
      "      editorial       0.51      0.46      0.48      1422\n",
      " belles_lettres       0.48      0.50      0.49      1186\n",
      "      adventure       0.60      0.61      0.60      1387\n",
      "        romance       0.58      0.41      0.48       515\n",
      "       religion       0.66      0.35      0.45       550\n",
      "science_fiction       0.43      0.48      0.45      1361\n",
      "          humor       0.60      0.32      0.41       266\n",
      "\n",
      "       accuracy                           0.54     17202\n",
      "      macro avg       0.56      0.49      0.51     17202\n",
      "   weighted avg       0.54      0.54      0.54     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 5.300689935684204]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 2, False, False] \n",
      " 0.13951866062085805 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.28      0.06      0.09      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.03      0.07      0.02     17202\n",
      "   weighted avg       0.05      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 10.47055697441101]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 2, False, False] \n",
      " 0.1926520172073015 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.17      0.14      0.15      1449\n",
      "        mystery       0.16      0.55      0.24      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.22      0.20      0.21       890\n",
      "        fiction       0.46      0.02      0.03      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.26      0.47      0.34      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.30      0.05      0.09      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.18      0.42      0.25      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.19     17202\n",
      "      macro avg       0.12      0.12      0.09     17202\n",
      "   weighted avg       0.15      0.19      0.13     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 5.070909023284912]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 2, False, False] \n",
      " 0.37449133821648645 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.27      0.42      0.33      1449\n",
      "        mystery       0.54      0.38      0.44      2121\n",
      "     government       0.28      0.23      0.25       904\n",
      "        hobbies       0.30      0.24      0.27      1268\n",
      "           news       0.53      0.55      0.54       890\n",
      "        fiction       0.62      0.47      0.54      1281\n",
      "        reviews       0.02      0.01      0.01       314\n",
      "           lore       0.69      0.61      0.65      2288\n",
      "      editorial       0.22      0.41      0.28      1422\n",
      " belles_lettres       0.21      0.14      0.17      1186\n",
      "      adventure       0.57      0.45      0.50      1387\n",
      "        romance       0.17      0.23      0.19       515\n",
      "       religion       0.12      0.14      0.13       550\n",
      "science_fiction       0.31      0.33      0.32      1361\n",
      "          humor       0.07      0.09      0.08       266\n",
      "\n",
      "       accuracy                           0.37     17202\n",
      "      macro avg       0.33      0.31      0.31     17202\n",
      "   weighted avg       0.40      0.37      0.38     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 126.38106298446655]\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 3, False, False] \n",
      " 0.4793047320079061 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.39      0.43      1449\n",
      "        mystery       0.44      0.57      0.49      2121\n",
      "     government       0.30      0.29      0.30       904\n",
      "        hobbies       0.41      0.37      0.39      1268\n",
      "           news       0.67      0.58      0.62       890\n",
      "        fiction       0.56      0.57      0.57      1281\n",
      "        reviews       0.22      0.23      0.22       314\n",
      "           lore       0.68      0.70      0.69      2288\n",
      "      editorial       0.48      0.36      0.42      1422\n",
      " belles_lettres       0.43      0.37      0.40      1186\n",
      "      adventure       0.48      0.60      0.53      1387\n",
      "        romance       0.41      0.45      0.43       515\n",
      "       religion       0.35      0.38      0.37       550\n",
      "science_fiction       0.40      0.39      0.39      1361\n",
      "          humor       0.41      0.28      0.33       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.45      0.43      0.44     17202\n",
      "   weighted avg       0.48      0.48      0.48     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.7784430980682373]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 3, False, False] \n",
      " 0.26898035112196256 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.17      0.25      1449\n",
      "        mystery       0.18      0.80      0.29      2121\n",
      "     government       1.00      0.01      0.02       904\n",
      "        hobbies       0.63      0.07      0.13      1268\n",
      "           news       0.83      0.08      0.15       890\n",
      "        fiction       0.91      0.06      0.12      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.33      0.84      0.47      2288\n",
      "      editorial       0.72      0.01      0.02      1422\n",
      " belles_lettres       0.58      0.05      0.09      1186\n",
      "      adventure       0.71      0.14      0.23      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.38      0.18      0.24      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.27     17202\n",
      "      macro avg       0.45      0.16      0.14     17202\n",
      "   weighted avg       0.51      0.27      0.19     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.7020890712738037]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 3, False, False] \n",
      " 0.432682246250436 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.41      0.44      0.43      1449\n",
      "        mystery       0.33      0.63      0.44      2121\n",
      "     government       0.52      0.12      0.19       904\n",
      "        hobbies       0.39      0.35      0.37      1268\n",
      "           news       0.71      0.46      0.56       890\n",
      "        fiction       0.55      0.47      0.51      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.46      0.77      0.58      2288\n",
      "      editorial       0.45      0.24      0.32      1422\n",
      " belles_lettres       0.45      0.34      0.39      1186\n",
      "      adventure       0.50      0.51      0.51      1387\n",
      "        romance       0.85      0.11      0.20       515\n",
      "       religion       0.77      0.04      0.07       550\n",
      "science_fiction       0.37      0.44      0.40      1361\n",
      "          humor       0.75      0.01      0.02       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.50      0.33      0.33     17202\n",
      "   weighted avg       0.47      0.43      0.40     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 155.17012310028076]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 3, False, False] \n",
      " 0.5286013254272759 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.45      0.49      0.47      1449\n",
      "        mystery       0.46      0.61      0.53      2121\n",
      "     government       0.51      0.28      0.36       904\n",
      "        hobbies       0.45      0.43      0.44      1268\n",
      "           news       0.70      0.65      0.67       890\n",
      "        fiction       0.62      0.62      0.62      1281\n",
      "        reviews       0.72      0.18      0.29       314\n",
      "           lore       0.61      0.77      0.68      2288\n",
      "      editorial       0.50      0.43      0.47      1422\n",
      " belles_lettres       0.48      0.48      0.48      1186\n",
      "      adventure       0.59      0.61      0.60      1387\n",
      "        romance       0.64      0.36      0.46       515\n",
      "       religion       0.69      0.30      0.42       550\n",
      "science_fiction       0.41      0.47      0.44      1361\n",
      "          humor       0.59      0.26      0.36       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.56      0.46      0.49     17202\n",
      "   weighted avg       0.54      0.53      0.52     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 8.471468925476074]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 3, False, False] \n",
      " 0.13765841181257993 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.21      0.04      0.07      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      0.99      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.02      0.07      0.02     17202\n",
      "   weighted avg       0.04      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 17.11476492881775]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 3, False, False] \n",
      " 0.1791652133472852 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.22      0.28      0.24      1449\n",
      "        mystery       0.13      0.51      0.21      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.50      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.23      0.59      0.33      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.17      0.17      0.17      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.08      0.10      0.06     17202\n",
      "   weighted avg       0.12      0.18      0.11     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 17.851143836975098]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 3, False, False] \n",
      " 0.3874549470991745 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.31      0.35      1449\n",
      "        mystery       0.61      0.35      0.44      2121\n",
      "     government       0.24      0.26      0.25       904\n",
      "        hobbies       0.25      0.38      0.30      1268\n",
      "           news       0.62      0.55      0.58       890\n",
      "        fiction       0.72      0.39      0.51      1281\n",
      "        reviews       0.04      0.10      0.06       314\n",
      "           lore       0.78      0.56      0.65      2288\n",
      "      editorial       0.27      0.44      0.33      1422\n",
      " belles_lettres       0.46      0.29      0.35      1186\n",
      "      adventure       0.63      0.44      0.52      1387\n",
      "        romance       0.17      0.29      0.21       515\n",
      "       religion       0.14      0.41      0.20       550\n",
      "science_fiction       0.36      0.35      0.36      1361\n",
      "          humor       0.09      0.12      0.10       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.39      0.35      0.35     17202\n",
      "   weighted avg       0.48      0.39      0.41     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 563.6405889987946]\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    X = V.fit_transform(brown['raw_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5413905359841878, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.535344727357284, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.5286013254272759, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.4856412045111034, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.4793047320079061, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.46918962911289386, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.4644808743169399, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.4629694221602139, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.432682246250436, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.41721892803162425, \"['Perceptron multicouche', 1, 1, False, False]\"]\n",
      "[0.3874549470991745, \"['Perceptron multicouche', 1, 3, False, False]\"]\n",
      "[0.37449133821648645, \"['Perceptron multicouche', 1, 2, False, False]\"]\n",
      "[0.3057202650854552, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.28618765259853507, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.26898035112196256, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.1926520172073015, \"['Decision Tree', 1, 2, False, False]\"]\n",
      "[0.18218811766073711, \"['Decision Tree', 1, 1, False, False]\"]\n",
      "[0.1791652133472852, \"['Decision Tree', 1, 3, False, False]\"]\n",
      "[0.1417277060806883, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.13765841181257993, \"['Random Forest', 1, 3, False, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "Perceptron 0.8964073944890129\n",
      "MultinomialNB 0.4001694155164682\n",
      "Logistic Regression 0.6609945687378544\n",
      "linear_svc 0.9301659275499526\n",
      "Random Forest 0.14689321839653197\n",
      "Decision Tree 0.17995415815436744\n",
      "Perceptron multicouche 0.7061388210673177\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "Perceptron 0.9825601674223927\n",
      "MultinomialNB 0.4022871094723205\n",
      "Logistic Regression 0.7295580248143904\n",
      "linear_svc 0.9857491653794409\n",
      "Random Forest 0.1445512980218247\n",
      "Decision Tree 0.18189745378444366\n",
      "Perceptron multicouche 0.7972744033085853\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "Perceptron 0.9878668593352933\n",
      "MultinomialNB 0.42503363396282823\n",
      "Logistic Regression 0.7654093377846429\n",
      "linear_svc 0.9904080920823161\n",
      "Random Forest 0.1433305097413922\n",
      "Decision Tree 0.17208131944790472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shailynnxie/Documents/Methodologie/projetFinal/brown.ipynb 儲存格 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/brown.ipynb#Y104sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNgram_range : (\u001b[39m\u001b[39m{\u001b[39;00mmin_N\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmax_N\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/brown.ipynb#Y104sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m nom, algo \u001b[39min\u001b[39;00m liste_classifieurs:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/brown.ipynb#Y104sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     clf \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/brown.ipynb#Y104sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     score \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/brown.ipynb#Y104sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(nom,score)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:752\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m    736\u001b[0m     \u001b[39m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \n\u001b[1;32m    738\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39m        Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 752\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, incremental\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:440\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39m# Run the LBFGS solver\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_lbfgs(\n\u001b[1;32m    441\u001b[0m         X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     iprint \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 536\u001b[0m opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m    537\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_grad_lbfgs,\n\u001b[1;32m    538\u001b[0m     packed_coef_inter,\n\u001b[1;32m    539\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    540\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    541\u001b[0m     options\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    542\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxfun\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_fun,\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint,\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    546\u001b[0m     },\n\u001b[1;32m    547\u001b[0m     args\u001b[39m=\u001b[39;49m(X, y, activations, deltas, coef_grads, intercept_grads),\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter)\n\u001b[1;32m    550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_ \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py:623\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    621\u001b[0m                               \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    622\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 623\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    624\u001b[0m                             callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    625\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mreturn\u001b[39;00m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    627\u001b[0m                          \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py:351\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    347\u001b[0m n_iterations \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    349\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[1;32m    350\u001b[0m     \u001b[39m# x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \\\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     _lbfgsb\u001b[39m.\u001b[39;49msetulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n\u001b[1;32m    352\u001b[0m                    pgtol, wa, iwa, task, iprint, csave, lsave,\n\u001b[1;32m    353\u001b[0m                    isave, dsave, maxls)\n\u001b[1;32m    354\u001b[0m     task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    356\u001b[0m         \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m         \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    358\u001b[0m         \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m         \u001b[39m# Overwrite f and g:\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    X = V.fit_transform(brown['raw_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        clf = algo.fit(X_train, y_train)\n",
    "        score = clf.score(X_train, y_train)\n",
    "        \n",
    "        print(nom,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chemin_expes = \"Pos_lower.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 1, False, True] \n",
      " 0.45907452621788164 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.39      0.43      1449\n",
      "        mystery       0.48      0.45      0.47      2121\n",
      "     government       0.27      0.39      0.32       904\n",
      "        hobbies       0.36      0.43      0.39      1268\n",
      "           news       0.52      0.60      0.56       890\n",
      "        fiction       0.58      0.53      0.56      1281\n",
      "        reviews       0.29      0.22      0.25       314\n",
      "           lore       0.65      0.64      0.65      2288\n",
      "      editorial       0.49      0.29      0.36      1422\n",
      " belles_lettres       0.44      0.42      0.43      1186\n",
      "      adventure       0.50      0.51      0.50      1387\n",
      "        romance       0.30      0.48      0.37       515\n",
      "       religion       0.41      0.30      0.35       550\n",
      "science_fiction       0.39      0.45      0.42      1361\n",
      "          humor       0.24      0.32      0.27       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.43      0.43      0.42     17202\n",
      "   weighted avg       0.47      0.46      0.46     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.7391388416290283]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 1, False, True] \n",
      " 0.3107196837577026 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.55      0.29      0.38      1449\n",
      "        mystery       0.19      0.82      0.31      2121\n",
      "     government       1.00      0.00      0.00       904\n",
      "        hobbies       0.71      0.13      0.22      1268\n",
      "           news       0.94      0.04      0.07       890\n",
      "        fiction       0.89      0.12      0.21      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.36      0.84      0.51      2288\n",
      "      editorial       0.71      0.07      0.12      1422\n",
      " belles_lettres       0.81      0.11      0.20      1186\n",
      "      adventure       0.67      0.25      0.36      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.42      0.25      0.31      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.31     17202\n",
      "      macro avg       0.48      0.19      0.18     17202\n",
      "   weighted avg       0.54      0.31      0.25     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.3741769790649414]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 1, False, True] \n",
      " 0.4658760609231485 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.48      0.45      1449\n",
      "        mystery       0.38      0.62      0.47      2121\n",
      "     government       0.48      0.17      0.25       904\n",
      "        hobbies       0.41      0.37      0.39      1268\n",
      "           news       0.64      0.56      0.59       890\n",
      "        fiction       0.57      0.55      0.56      1281\n",
      "        reviews       0.93      0.04      0.09       314\n",
      "           lore       0.53      0.75      0.62      2288\n",
      "      editorial       0.43      0.32      0.37      1422\n",
      " belles_lettres       0.45      0.39      0.42      1186\n",
      "      adventure       0.52      0.53      0.53      1387\n",
      "        romance       0.63      0.18      0.28       515\n",
      "       religion       0.64      0.15      0.25       550\n",
      "science_fiction       0.40      0.44      0.42      1361\n",
      "          humor       0.88      0.06      0.11       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.55      0.37      0.38     17202\n",
      "   weighted avg       0.49      0.47      0.45     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 7.93510890007019]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 1, False, True] \n",
      " 0.5331356818974538 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.49      0.48      1449\n",
      "        mystery       0.47      0.57      0.52      2121\n",
      "     government       0.47      0.32      0.38       904\n",
      "        hobbies       0.47      0.44      0.46      1268\n",
      "           news       0.63      0.66      0.65       890\n",
      "        fiction       0.63      0.62      0.62      1281\n",
      "        reviews       0.57      0.25      0.34       314\n",
      "           lore       0.66      0.76      0.70      2288\n",
      "      editorial       0.48      0.45      0.47      1422\n",
      " belles_lettres       0.47      0.49      0.48      1186\n",
      "      adventure       0.58      0.58      0.58      1387\n",
      "        romance       0.56      0.43      0.49       515\n",
      "       religion       0.60      0.36      0.45       550\n",
      "science_fiction       0.44      0.49      0.47      1361\n",
      "          humor       0.56      0.31      0.40       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.54      0.48      0.50     17202\n",
      "   weighted avg       0.53      0.53      0.53     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 2.3845131397247314]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 1, False, True] \n",
      " 0.14225090105801652 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.20      0.08      0.12      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      0.99      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       1.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.09      0.07      0.02     17202\n",
      "   weighted avg       0.12      0.14      0.05     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 6.3884570598602295]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 1, False, True] \n",
      " 0.17782815951633532 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.14      0.78      0.23      1449\n",
      "        mystery       0.18      0.20      0.19      2121\n",
      "     government       0.38      0.02      0.03       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.26      0.10      0.14       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.23      0.58      0.33      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.20      0.06      0.09      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.16      0.12      0.07     17202\n",
      "   weighted avg       0.19      0.18      0.10     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.9760608673095703]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 1, False, True] \n",
      " 0.4180327868852459 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.36      0.40      0.38      1449\n",
      "        mystery       0.46      0.48      0.47      2121\n",
      "     government       0.24      0.28      0.26       904\n",
      "        hobbies       0.24      0.34      0.28      1268\n",
      "           news       0.55      0.58      0.57       890\n",
      "        fiction       0.56      0.55      0.55      1281\n",
      "        reviews       0.04      0.00      0.01       314\n",
      "           lore       0.65      0.65      0.65      2288\n",
      "      editorial       0.30      0.36      0.33      1422\n",
      " belles_lettres       0.38      0.37      0.38      1186\n",
      "      adventure       0.53      0.45      0.48      1387\n",
      "        romance       0.31      0.30      0.30       515\n",
      "       religion       0.31      0.16      0.21       550\n",
      "science_fiction       0.33      0.25      0.28      1361\n",
      "          humor       0.16      0.08      0.11       266\n",
      "\n",
      "       accuracy                           0.42     17202\n",
      "      macro avg       0.36      0.35      0.35     17202\n",
      "   weighted avg       0.42      0.42      0.41     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 28.259538173675537]\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 2, False, True] \n",
      " 0.48378095570282525 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.42      0.44      1449\n",
      "        mystery       0.47      0.51      0.49      2121\n",
      "     government       0.40      0.31      0.35       904\n",
      "        hobbies       0.40      0.40      0.40      1268\n",
      "           news       0.65      0.60      0.62       890\n",
      "        fiction       0.61      0.54      0.57      1281\n",
      "        reviews       0.18      0.25      0.21       314\n",
      "           lore       0.62      0.71      0.66      2288\n",
      "      editorial       0.43      0.42      0.43      1422\n",
      " belles_lettres       0.48      0.39      0.43      1186\n",
      "      adventure       0.51      0.59      0.55      1387\n",
      "        romance       0.46      0.35      0.40       515\n",
      "       religion       0.39      0.39      0.39       550\n",
      "science_fiction       0.40      0.40      0.40      1361\n",
      "          humor       0.30      0.30      0.30       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.45      0.44      0.44     17202\n",
      "   weighted avg       0.48      0.48      0.48     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.2203459739685059]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 2, False, True] \n",
      " 0.28874549470991745 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.53      0.22      0.31      1449\n",
      "        mystery       0.18      0.82      0.30      2121\n",
      "     government       1.00      0.00      0.00       904\n",
      "        hobbies       0.71      0.10      0.17      1268\n",
      "           news       0.87      0.07      0.13       890\n",
      "        fiction       0.92      0.09      0.16      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.35      0.83      0.49      2288\n",
      "      editorial       0.78      0.03      0.06      1422\n",
      " belles_lettres       0.74      0.07      0.13      1186\n",
      "      adventure       0.68      0.20      0.31      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.40      0.22      0.28      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.29     17202\n",
      "      macro avg       0.48      0.18      0.16     17202\n",
      "   weighted avg       0.53      0.29      0.22     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.46368885040283203]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 2, False, True] \n",
      " 0.46750377863039183 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.49      0.45      1449\n",
      "        mystery       0.37      0.61      0.46      2121\n",
      "     government       0.53      0.16      0.24       904\n",
      "        hobbies       0.42      0.36      0.39      1268\n",
      "           news       0.73      0.53      0.61       890\n",
      "        fiction       0.56      0.56      0.56      1281\n",
      "        reviews       1.00      0.03      0.06       314\n",
      "           lore       0.53      0.78      0.63      2288\n",
      "      editorial       0.43      0.32      0.37      1422\n",
      " belles_lettres       0.45      0.39      0.42      1186\n",
      "      adventure       0.52      0.56      0.54      1387\n",
      "        romance       0.75      0.18      0.29       515\n",
      "       religion       0.72      0.11      0.19       550\n",
      "science_fiction       0.39      0.45      0.42      1361\n",
      "          humor       0.86      0.02      0.04       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.58      0.37      0.38     17202\n",
      "   weighted avg       0.51      0.47      0.45     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 34.58942794799805]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 2, False, True] \n",
      " 0.5412161376584118 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.51      0.49      1449\n",
      "        mystery       0.48      0.60      0.53      2121\n",
      "     government       0.51      0.31      0.39       904\n",
      "        hobbies       0.45      0.44      0.44      1268\n",
      "           news       0.68      0.67      0.67       890\n",
      "        fiction       0.64      0.64      0.64      1281\n",
      "        reviews       0.65      0.25      0.36       314\n",
      "           lore       0.65      0.77      0.70      2288\n",
      "      editorial       0.50      0.46      0.48      1422\n",
      " belles_lettres       0.47      0.50      0.48      1186\n",
      "      adventure       0.59      0.61      0.60      1387\n",
      "        romance       0.57      0.40      0.47       515\n",
      "       religion       0.65      0.34      0.44       550\n",
      "science_fiction       0.44      0.48      0.46      1361\n",
      "          humor       0.63      0.32      0.43       266\n",
      "\n",
      "       accuracy                           0.54     17202\n",
      "      macro avg       0.56      0.49      0.51     17202\n",
      "   weighted avg       0.54      0.54      0.54     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 5.196425199508667]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 2, False, True] \n",
      " 0.14085571445180792 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.21      0.07      0.10      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.02      0.07      0.02     17202\n",
      "   weighted avg       0.04      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 10.20895004272461]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 2, False, True] \n",
      " 0.19090803394954076 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.22      0.06      0.10      1449\n",
      "        mystery       0.14      0.17      0.15      2121\n",
      "     government       1.00      0.00      0.01       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.44      0.07      0.13       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.21      0.73      0.33      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.20      0.25      0.22      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.16      0.56      0.25      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.19     17202\n",
      "      macro avg       0.23      0.12      0.08     17202\n",
      "   weighted avg       0.24      0.19      0.12     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 4.749161005020142]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 2, False, True] \n",
      " 0.4040227880479014 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.34      0.41      1449\n",
      "        mystery       0.49      0.45      0.47      2121\n",
      "     government       0.14      0.24      0.18       904\n",
      "        hobbies       0.26      0.33      0.29      1268\n",
      "           news       0.55      0.61      0.57       890\n",
      "        fiction       0.62      0.44      0.52      1281\n",
      "        reviews       0.09      0.11      0.10       314\n",
      "           lore       0.74      0.62      0.67      2288\n",
      "      editorial       0.35      0.38      0.37      1422\n",
      " belles_lettres       0.36      0.39      0.38      1186\n",
      "      adventure       0.49      0.46      0.47      1387\n",
      "        romance       0.12      0.24      0.16       515\n",
      "       religion       0.13      0.11      0.12       550\n",
      "science_fiction       0.36      0.33      0.35      1361\n",
      "          humor       0.19      0.11      0.14       266\n",
      "\n",
      "       accuracy                           0.40     17202\n",
      "      macro avg       0.36      0.34      0.35     17202\n",
      "   weighted avg       0.44      0.40      0.42     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 115.06333804130554]\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 3, False, True] \n",
      " 0.48069991861411465 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.34      0.41      1449\n",
      "        mystery       0.44      0.53      0.48      2121\n",
      "     government       0.37      0.30      0.34       904\n",
      "        hobbies       0.38      0.38      0.38      1268\n",
      "           news       0.60      0.63      0.61       890\n",
      "        fiction       0.53      0.58      0.55      1281\n",
      "        reviews       0.25      0.22      0.23       314\n",
      "           lore       0.63      0.73      0.68      2288\n",
      "      editorial       0.48      0.37      0.42      1422\n",
      " belles_lettres       0.42      0.44      0.43      1186\n",
      "      adventure       0.52      0.55      0.54      1387\n",
      "        romance       0.48      0.39      0.43       515\n",
      "       religion       0.36      0.35      0.35       550\n",
      "science_fiction       0.39      0.41      0.40      1361\n",
      "          humor       0.37      0.28      0.32       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.45      0.43      0.44     17202\n",
      "   weighted avg       0.48      0.48      0.48     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.8752148151397705]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 3, False, True] \n",
      " 0.270026741076619 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.17      0.25      1449\n",
      "        mystery       0.18      0.80      0.29      2121\n",
      "     government       1.00      0.01      0.02       904\n",
      "        hobbies       0.65      0.07      0.13      1268\n",
      "           news       0.84      0.08      0.15       890\n",
      "        fiction       0.90      0.06      0.12      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.33      0.84      0.48      2288\n",
      "      editorial       0.70      0.01      0.03      1422\n",
      " belles_lettres       0.59      0.05      0.09      1186\n",
      "      adventure       0.70      0.14      0.24      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.39      0.18      0.25      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.27     17202\n",
      "      macro avg       0.45      0.16      0.14     17202\n",
      "   weighted avg       0.51      0.27      0.20     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.7326362133026123]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 3, False, True] \n",
      " 0.4296593419369841 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.38      0.50      0.43      1449\n",
      "        mystery       0.34      0.61      0.43      2121\n",
      "     government       0.57      0.10      0.17       904\n",
      "        hobbies       0.38      0.33      0.36      1268\n",
      "           news       0.70      0.47      0.56       890\n",
      "        fiction       0.56      0.46      0.50      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.46      0.78      0.58      2288\n",
      "      editorial       0.43      0.25      0.32      1422\n",
      " belles_lettres       0.47      0.33      0.39      1186\n",
      "      adventure       0.48      0.52      0.50      1387\n",
      "        romance       0.78      0.11      0.19       515\n",
      "       religion       0.79      0.03      0.07       550\n",
      "science_fiction       0.38      0.40      0.39      1361\n",
      "          humor       1.00      0.01      0.01       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.51      0.33      0.33     17202\n",
      "   weighted avg       0.47      0.43      0.40     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 100.22714591026306]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 3, False, True] \n",
      " 0.5297639809324497 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.51      0.49      1449\n",
      "        mystery       0.45      0.60      0.52      2121\n",
      "     government       0.53      0.29      0.38       904\n",
      "        hobbies       0.45      0.43      0.44      1268\n",
      "           news       0.69      0.66      0.68       890\n",
      "        fiction       0.62      0.63      0.63      1281\n",
      "        reviews       0.67      0.18      0.29       314\n",
      "           lore       0.62      0.78      0.69      2288\n",
      "      editorial       0.50      0.43      0.46      1422\n",
      " belles_lettres       0.47      0.46      0.47      1186\n",
      "      adventure       0.58      0.61      0.59      1387\n",
      "        romance       0.60      0.36      0.45       515\n",
      "       religion       0.67      0.30      0.41       550\n",
      "science_fiction       0.42      0.46      0.44      1361\n",
      "          humor       0.62      0.26      0.36       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.56      0.46      0.49     17202\n",
      "   weighted avg       0.54      0.53      0.52     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 8.606985092163086]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 3, False, True] \n",
      " 0.13754214626206254 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.44      0.04      0.07      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.04      0.07      0.02     17202\n",
      "   weighted avg       0.07      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 17.384557723999023]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 3, False, True] \n",
      " 0.19137309615161027 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.16      0.63      0.25      1449\n",
      "        mystery       0.18      0.16      0.17      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.43      0.06      0.11       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.21      0.69      0.32      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.78      0.01      0.01      1186\n",
      "      adventure       0.23      0.26      0.24      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.25      0.03      0.05      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.19     17202\n",
      "      macro avg       0.15      0.12      0.08     17202\n",
      "   weighted avg       0.18      0.19      0.11     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 17.42638897895813]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 3, False, True] \n",
      " 0.3730380188350192 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.49      0.34      0.40      1449\n",
      "        mystery       0.48      0.40      0.44      2121\n",
      "     government       0.24      0.23      0.23       904\n",
      "        hobbies       0.32      0.41      0.36      1268\n",
      "           news       0.66      0.47      0.55       890\n",
      "        fiction       0.39      0.46      0.42      1281\n",
      "        reviews       0.06      0.11      0.08       314\n",
      "           lore       0.72      0.58      0.64      2288\n",
      "      editorial       0.40      0.24      0.30      1422\n",
      " belles_lettres       0.43      0.26      0.32      1186\n",
      "      adventure       0.68      0.39      0.49      1387\n",
      "        romance       0.12      0.26      0.17       515\n",
      "       religion       0.11      0.35      0.16       550\n",
      "science_fiction       0.34      0.31      0.32      1361\n",
      "          humor       0.04      0.14      0.06       266\n",
      "\n",
      "       accuracy                           0.37     17202\n",
      "      macro avg       0.36      0.33      0.33     17202\n",
      "   weighted avg       0.45      0.37      0.40     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 566.636715888977]\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N))\n",
    "    X = V.fit_transform(brown['raw_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            print(calcul_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5412161376584118, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.5331356818974538, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.5297639809324497, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.48378095570282525, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.48069991861411465, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.46750377863039183, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.4658760609231485, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.45907452621788164, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.4296593419369841, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.4180327868852459, \"['Perceptron multicouche', 1, 1, False, True]\"]\n",
      "[0.4040227880479014, \"['Perceptron multicouche', 1, 2, False, True]\"]\n",
      "[0.3730380188350192, \"['Perceptron multicouche', 1, 3, False, True]\"]\n",
      "[0.3107196837577026, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.28874549470991745, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.270026741076619, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.19137309615161027, \"['Decision Tree', 1, 3, False, True]\"]\n",
      "[0.19090803394954076, \"['Decision Tree', 1, 2, False, True]\"]\n",
      "[0.17782815951633532, \"['Decision Tree', 1, 1, False, True]\"]\n",
      "[0.14225090105801652, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.14085571445180792, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.13754214626206254, \"['Random Forest', 1, 3, False, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_minuscules,enlever_stopwords  = True, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N))\n",
    "    X = V.fit_transform(brown['raw_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "       \n",
    "        clf = algo.fit(X_train, y_train)\n",
    "        score = clf.score(X_train, y_train)\n",
    "\n",
    "        print(nom,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"Pos_lower_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 1, True, True] \n",
      " 0.4611091733519358 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.46      0.44      1449\n",
      "        mystery       0.38      0.57      0.46      2121\n",
      "     government       0.34      0.31      0.33       904\n",
      "        hobbies       0.40      0.35      0.37      1268\n",
      "           news       0.66      0.49      0.56       890\n",
      "        fiction       0.50      0.59      0.54      1281\n",
      "        reviews       0.35      0.19      0.25       314\n",
      "           lore       0.67      0.63      0.65      2288\n",
      "      editorial       0.49      0.35      0.41      1422\n",
      " belles_lettres       0.38      0.44      0.41      1186\n",
      "      adventure       0.53      0.49      0.51      1387\n",
      "        romance       0.39      0.34      0.36       515\n",
      "       religion       0.50      0.27      0.35       550\n",
      "science_fiction       0.39      0.40      0.39      1361\n",
      "          humor       0.33      0.27      0.30       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.45      0.41      0.42     17202\n",
      "   weighted avg       0.47      0.46      0.46     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.6550631523132324]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 1, True, True] \n",
      " 0.3249040809208232 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.54      0.32      0.40      1449\n",
      "        mystery       0.20      0.83      0.32      2121\n",
      "     government       1.00      0.00      0.01       904\n",
      "        hobbies       0.70      0.15      0.24      1268\n",
      "           news       0.96      0.06      0.11       890\n",
      "        fiction       0.88      0.14      0.25      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.37      0.83      0.51      2288\n",
      "      editorial       0.71      0.09      0.15      1422\n",
      " belles_lettres       0.76      0.13      0.22      1186\n",
      "      adventure       0.65      0.29      0.40      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.41      0.27      0.33      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.32     17202\n",
      "      macro avg       0.48      0.21      0.20     17202\n",
      "   weighted avg       0.54      0.32      0.27     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.38156580924987793]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 1, True, True] \n",
      " 0.47058481571910243 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.44      0.48      0.46      1449\n",
      "        mystery       0.38      0.60      0.47      2121\n",
      "     government       0.50      0.18      0.26       904\n",
      "        hobbies       0.42      0.38      0.40      1268\n",
      "           news       0.63      0.53      0.58       890\n",
      "        fiction       0.57      0.56      0.57      1281\n",
      "        reviews       0.89      0.05      0.10       314\n",
      "           lore       0.54      0.76      0.63      2288\n",
      "      editorial       0.43      0.34      0.38      1422\n",
      " belles_lettres       0.44      0.40      0.42      1186\n",
      "      adventure       0.51      0.53      0.52      1387\n",
      "        romance       0.63      0.19      0.30       515\n",
      "       religion       0.61      0.16      0.25       550\n",
      "science_fiction       0.39      0.45      0.42      1361\n",
      "          humor       0.88      0.05      0.10       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.55      0.38      0.39     17202\n",
      "   weighted avg       0.49      0.47      0.45     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 7.986335039138794]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 1, True, True] \n",
      " 0.5330775491221951 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.50      0.49      1449\n",
      "        mystery       0.47      0.58      0.52      2121\n",
      "     government       0.47      0.32      0.38       904\n",
      "        hobbies       0.47      0.45      0.46      1268\n",
      "           news       0.62      0.66      0.64       890\n",
      "        fiction       0.62      0.62      0.62      1281\n",
      "        reviews       0.56      0.25      0.34       314\n",
      "           lore       0.66      0.75      0.70      2288\n",
      "      editorial       0.48      0.45      0.46      1422\n",
      " belles_lettres       0.47      0.48      0.48      1186\n",
      "      adventure       0.58      0.57      0.58      1387\n",
      "        romance       0.55      0.44      0.49       515\n",
      "       religion       0.58      0.36      0.45       550\n",
      "science_fiction       0.44      0.50      0.47      1361\n",
      "          humor       0.59      0.31      0.41       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.54      0.48      0.50     17202\n",
      "   weighted avg       0.53      0.53      0.53     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 2.1604387760162354]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 1, True, True] \n",
      " 0.143471689338449 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.14      0.09      0.11      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      0.99      0.25      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.02      0.07      0.02     17202\n",
      "   weighted avg       0.04      0.14      0.05     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 6.33645486831665]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 1, True, True] \n",
      " 0.1772468317637484 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.19      0.29      0.23      1449\n",
      "        mystery       0.14      0.23      0.18      2121\n",
      "     government       1.00      0.00      0.01       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.35      0.03      0.06       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.21      0.71      0.33      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.72      0.01      0.02      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.12      0.34      0.18      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.25      0.11      0.07     17202\n",
      "   weighted avg       0.27      0.18      0.10     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.9701449871063232]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 1, True, True] \n",
      " 0.42285780723171723 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.41      0.45      0.43      1449\n",
      "        mystery       0.46      0.48      0.47      2121\n",
      "     government       0.21      0.12      0.16       904\n",
      "        hobbies       0.32      0.33      0.33      1268\n",
      "           news       0.48      0.58      0.53       890\n",
      "        fiction       0.56      0.54      0.55      1281\n",
      "        reviews       0.18      0.02      0.04       314\n",
      "           lore       0.63      0.65      0.64      2288\n",
      "      editorial       0.27      0.37      0.31      1422\n",
      " belles_lettres       0.34      0.38      0.36      1186\n",
      "      adventure       0.51      0.49      0.50      1387\n",
      "        romance       0.25      0.21      0.23       515\n",
      "       religion       0.23      0.13      0.17       550\n",
      "science_fiction       0.37      0.41      0.39      1361\n",
      "          humor       0.03      0.01      0.02       266\n",
      "\n",
      "       accuracy                           0.42     17202\n",
      "      macro avg       0.35      0.34      0.34     17202\n",
      "   weighted avg       0.41      0.42      0.41     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 29.82099199295044]\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 2, True, True] \n",
      " 0.4842460179048948 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.46      0.40      0.42      1449\n",
      "        mystery       0.46      0.53      0.49      2121\n",
      "     government       0.36      0.33      0.34       904\n",
      "        hobbies       0.41      0.37      0.39      1268\n",
      "           news       0.66      0.60      0.63       890\n",
      "        fiction       0.54      0.59      0.57      1281\n",
      "        reviews       0.24      0.32      0.27       314\n",
      "           lore       0.66      0.69      0.68      2288\n",
      "      editorial       0.43      0.40      0.42      1422\n",
      " belles_lettres       0.42      0.48      0.45      1186\n",
      "      adventure       0.53      0.54      0.54      1387\n",
      "        romance       0.42      0.41      0.41       515\n",
      "       religion       0.37      0.38      0.38       550\n",
      "science_fiction       0.44      0.35      0.39      1361\n",
      "          humor       0.32      0.33      0.33       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.45      0.45      0.45     17202\n",
      "   weighted avg       0.48      0.48      0.48     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.2092349529266357]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 2, True, True] \n",
      " 0.2960120916172538 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.53      0.24      0.33      1449\n",
      "        mystery       0.19      0.83      0.30      2121\n",
      "     government       1.00      0.00      0.01       904\n",
      "        hobbies       0.71      0.11      0.19      1268\n",
      "           news       0.86      0.07      0.14       890\n",
      "        fiction       0.92      0.10      0.19      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.36      0.83      0.50      2288\n",
      "      editorial       0.68      0.04      0.07      1422\n",
      " belles_lettres       0.74      0.08      0.15      1186\n",
      "      adventure       0.68      0.21      0.33      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.39      0.23      0.29      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.30     17202\n",
      "      macro avg       0.47      0.18      0.17     17202\n",
      "   weighted avg       0.53      0.30      0.23     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.46683168411254883]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 2, True, True] \n",
      " 0.46558539704685503 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.41      0.50      0.45      1449\n",
      "        mystery       0.37      0.62      0.46      2121\n",
      "     government       0.52      0.17      0.26       904\n",
      "        hobbies       0.42      0.35      0.39      1268\n",
      "           news       0.69      0.54      0.61       890\n",
      "        fiction       0.57      0.55      0.56      1281\n",
      "        reviews       1.00      0.02      0.04       314\n",
      "           lore       0.53      0.77      0.62      2288\n",
      "      editorial       0.45      0.33      0.38      1422\n",
      " belles_lettres       0.45      0.40      0.42      1186\n",
      "      adventure       0.50      0.55      0.52      1387\n",
      "        romance       0.74      0.17      0.27       515\n",
      "       religion       0.72      0.11      0.18       550\n",
      "science_fiction       0.39      0.43      0.41      1361\n",
      "          humor       0.86      0.02      0.04       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.57      0.37      0.38     17202\n",
      "   weighted avg       0.50      0.47      0.44     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 32.51113414764404]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 2, True, True] \n",
      " 0.5411580048831531 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.51      0.49      1449\n",
      "        mystery       0.48      0.60      0.54      2121\n",
      "     government       0.51      0.31      0.39       904\n",
      "        hobbies       0.45      0.43      0.44      1268\n",
      "           news       0.67      0.67      0.67       890\n",
      "        fiction       0.64      0.64      0.64      1281\n",
      "        reviews       0.63      0.24      0.35       314\n",
      "           lore       0.65      0.77      0.71      2288\n",
      "      editorial       0.51      0.46      0.48      1422\n",
      " belles_lettres       0.47      0.49      0.48      1186\n",
      "      adventure       0.59      0.61      0.60      1387\n",
      "        romance       0.56      0.41      0.47       515\n",
      "       religion       0.64      0.33      0.44       550\n",
      "science_fiction       0.44      0.48      0.46      1361\n",
      "          humor       0.61      0.32      0.42       266\n",
      "\n",
      "       accuracy                           0.54     17202\n",
      "      macro avg       0.56      0.49      0.51     17202\n",
      "   weighted avg       0.54      0.54      0.54     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 5.075222015380859]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 2, True, True] \n",
      " 0.13841413789094292 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.29      0.05      0.08      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.03      0.07      0.02     17202\n",
      "   weighted avg       0.05      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 10.212449073791504]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 2, True, True] \n",
      " 0.17550284850598769 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.16      0.04      0.06      1449\n",
      "        mystery       0.15      0.19      0.17      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.37      0.02      0.05       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.19      0.70      0.30      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.28      0.09      0.13      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.15      0.59      0.24      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.15      0.11      0.06     17202\n",
      "   weighted avg       0.19      0.18      0.10     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 4.736188888549805]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 2, True, True] \n",
      " 0.35280781304499476 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.54      0.26      0.35      1449\n",
      "        mystery       0.37      0.50      0.42      2121\n",
      "     government       0.13      0.11      0.12       904\n",
      "        hobbies       0.21      0.38      0.27      1268\n",
      "           news       0.35      0.50      0.41       890\n",
      "        fiction       0.45      0.37      0.40      1281\n",
      "        reviews       0.02      0.00      0.01       314\n",
      "           lore       0.68      0.57      0.62      2288\n",
      "      editorial       0.21      0.35      0.26      1422\n",
      " belles_lettres       0.31      0.29      0.30      1186\n",
      "      adventure       0.45      0.38      0.41      1387\n",
      "        romance       0.14      0.03      0.05       515\n",
      "       religion       0.07      0.01      0.02       550\n",
      "science_fiction       0.29      0.34      0.31      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.35     17202\n",
      "      macro avg       0.28      0.27      0.26     17202\n",
      "   weighted avg       0.36      0.35      0.34     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 117.40093231201172]\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 3, True, True] \n",
      " 0.47552610161609116 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.44      0.40      0.42      1449\n",
      "        mystery       0.46      0.52      0.49      2121\n",
      "     government       0.36      0.26      0.30       904\n",
      "        hobbies       0.41      0.35      0.38      1268\n",
      "           news       0.64      0.61      0.63       890\n",
      "        fiction       0.53      0.56      0.55      1281\n",
      "        reviews       0.21      0.28      0.24       314\n",
      "           lore       0.60      0.73      0.66      2288\n",
      "      editorial       0.44      0.38      0.41      1422\n",
      " belles_lettres       0.42      0.42      0.42      1186\n",
      "      adventure       0.53      0.54      0.53      1387\n",
      "        romance       0.41      0.38      0.40       515\n",
      "       religion       0.40      0.35      0.37       550\n",
      "science_fiction       0.39      0.40      0.40      1361\n",
      "          humor       0.40      0.28      0.33       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.44      0.43      0.43     17202\n",
      "   weighted avg       0.47      0.48      0.47     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.8219568729400635]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 3, True, True] \n",
      " 0.27589815137774676 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.19      0.28      1449\n",
      "        mystery       0.18      0.80      0.29      2121\n",
      "     government       1.00      0.01      0.02       904\n",
      "        hobbies       0.65      0.08      0.14      1268\n",
      "           news       0.83      0.08      0.15       890\n",
      "        fiction       0.89      0.07      0.13      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.34      0.84      0.48      2288\n",
      "      editorial       0.68      0.02      0.04      1422\n",
      " belles_lettres       0.61      0.06      0.11      1186\n",
      "      adventure       0.70      0.16      0.26      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.40      0.20      0.26      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.28     17202\n",
      "      macro avg       0.45      0.17      0.14     17202\n",
      "   weighted avg       0.51      0.28      0.20     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.716094970703125]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 3, True, True] \n",
      " 0.43680967329380305 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.48      0.43      1449\n",
      "        mystery       0.34      0.61      0.44      2121\n",
      "     government       0.57      0.11      0.19       904\n",
      "        hobbies       0.40      0.33      0.36      1268\n",
      "           news       0.70      0.48      0.57       890\n",
      "        fiction       0.55      0.48      0.51      1281\n",
      "        reviews       1.00      0.01      0.02       314\n",
      "           lore       0.47      0.78      0.59      2288\n",
      "      editorial       0.47      0.27      0.34      1422\n",
      " belles_lettres       0.46      0.34      0.39      1186\n",
      "      adventure       0.48      0.52      0.50      1387\n",
      "        romance       0.78      0.11      0.20       515\n",
      "       religion       0.79      0.04      0.08       550\n",
      "science_fiction       0.37      0.43      0.40      1361\n",
      "          humor       1.00      0.01      0.01       266\n",
      "\n",
      "       accuracy                           0.44     17202\n",
      "      macro avg       0.58      0.33      0.34     17202\n",
      "   weighted avg       0.50      0.44      0.41     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 99.71388602256775]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 3, True, True] \n",
      " 0.529880246482967 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.51      0.49      1449\n",
      "        mystery       0.46      0.60      0.52      2121\n",
      "     government       0.53      0.29      0.38       904\n",
      "        hobbies       0.44      0.43      0.44      1268\n",
      "           news       0.70      0.66      0.68       890\n",
      "        fiction       0.62      0.63      0.62      1281\n",
      "        reviews       0.64      0.18      0.29       314\n",
      "           lore       0.62      0.78      0.69      2288\n",
      "      editorial       0.50      0.44      0.47      1422\n",
      " belles_lettres       0.47      0.46      0.47      1186\n",
      "      adventure       0.58      0.60      0.59      1387\n",
      "        romance       0.60      0.36      0.45       515\n",
      "       religion       0.66      0.30      0.41       550\n",
      "science_fiction       0.42      0.46      0.44      1361\n",
      "          humor       0.62      0.25      0.36       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.56      0.46      0.49     17202\n",
      "   weighted avg       0.54      0.53      0.52     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 8.38128113746643]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 3, True, True] \n",
      " 0.1373096151610278 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.42      0.04      0.07      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.04      0.07      0.02     17202\n",
      "   weighted avg       0.07      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 17.230241060256958]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 3, True, True] \n",
      " 0.18149052435763283 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.18      0.24      0.21      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.34      0.08      0.12       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.20      0.71      0.31      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.27      0.06      0.10      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.15      0.61      0.24      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.08      0.11      0.06     17202\n",
      "   weighted avg       0.10      0.18      0.10     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 17.410807847976685]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 3, True, True] \n",
      " 0.3768166492268341 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.45      0.32      0.37      1449\n",
      "        mystery       0.51      0.33      0.40      2121\n",
      "     government       0.23      0.19      0.21       904\n",
      "        hobbies       0.56      0.21      0.30      1268\n",
      "           news       0.63      0.43      0.51       890\n",
      "        fiction       0.60      0.37      0.46      1281\n",
      "        reviews       0.07      0.05      0.06       314\n",
      "           lore       0.39      0.77      0.52      2288\n",
      "      editorial       0.49      0.23      0.31      1422\n",
      " belles_lettres       0.33      0.25      0.29      1186\n",
      "      adventure       0.57      0.49      0.53      1387\n",
      "        romance       0.08      0.14      0.10       515\n",
      "       religion       0.24      0.18      0.21       550\n",
      "science_fiction       0.23      0.56      0.33      1361\n",
      "          humor       0.05      0.02      0.03       266\n",
      "\n",
      "       accuracy                           0.38     17202\n",
      "      macro avg       0.36      0.30      0.31     17202\n",
      "   weighted avg       0.42      0.38      0.37     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 530.7928931713104]\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),max_df = 0.7)\n",
    "    X = V.fit_transform(brown['raw_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            #print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5411580048831531, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.5330775491221951, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.529880246482967, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.4842460179048948, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.47552610161609116, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.47058481571910243, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.46558539704685503, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.4611091733519358, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.43680967329380305, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.42285780723171723, \"['Perceptron multicouche', 1, 1, True, True]\"]\n",
      "[0.3768166492268341, \"['Perceptron multicouche', 1, 3, True, True]\"]\n",
      "[0.35280781304499476, \"['Perceptron multicouche', 1, 2, True, True]\"]\n",
      "[0.3249040809208232, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.2960120916172538, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.27589815137774676, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.18149052435763283, \"['Decision Tree', 1, 3, True, True]\"]\n",
      "[0.1772468317637484, \"['Decision Tree', 1, 1, True, True]\"]\n",
      "[0.17550284850598769, \"['Decision Tree', 1, 2, True, True]\"]\n",
      "[0.143471689338449, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.13841413789094292, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.1373096151610278, \"['Random Forest', 1, 3, True, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"Pos_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 1, True, False] \n",
      " 0.46407394489012904 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.50      0.40      0.44      1449\n",
      "        mystery       0.38      0.54      0.45      2121\n",
      "     government       0.32      0.29      0.30       904\n",
      "        hobbies       0.41      0.39      0.40      1268\n",
      "           news       0.62      0.57      0.59       890\n",
      "        fiction       0.54      0.55      0.55      1281\n",
      "        reviews       0.19      0.26      0.22       314\n",
      "           lore       0.70      0.64      0.67      2288\n",
      "      editorial       0.50      0.34      0.40      1422\n",
      " belles_lettres       0.43      0.44      0.43      1186\n",
      "      adventure       0.45      0.53      0.49      1387\n",
      "        romance       0.39      0.38      0.38       515\n",
      "       religion       0.46      0.31      0.37       550\n",
      "science_fiction       0.42      0.40      0.41      1361\n",
      "          humor       0.24      0.35      0.29       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.44      0.43      0.43     17202\n",
      "   weighted avg       0.48      0.46      0.47     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.7898180484771729]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 1, True, False] \n",
      " 0.3203115916753866 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.54      0.31      0.39      1449\n",
      "        mystery       0.20      0.83      0.32      2121\n",
      "     government       1.00      0.00      0.01       904\n",
      "        hobbies       0.68      0.13      0.22      1268\n",
      "           news       0.94      0.08      0.14       890\n",
      "        fiction       0.89      0.12      0.22      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.37      0.84      0.51      2288\n",
      "      editorial       0.75      0.07      0.14      1422\n",
      " belles_lettres       0.80      0.13      0.23      1186\n",
      "      adventure       0.67      0.28      0.39      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.41      0.26      0.31      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.32     17202\n",
      "      macro avg       0.48      0.20      0.19     17202\n",
      "   weighted avg       0.54      0.32      0.26     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.38893914222717285]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 1, True, False] \n",
      " 0.4688408324613417 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.48      0.45      1449\n",
      "        mystery       0.38      0.62      0.47      2121\n",
      "     government       0.52      0.18      0.27       904\n",
      "        hobbies       0.43      0.37      0.39      1268\n",
      "           news       0.61      0.55      0.58       890\n",
      "        fiction       0.55      0.55      0.55      1281\n",
      "        reviews       0.93      0.04      0.09       314\n",
      "           lore       0.53      0.76      0.62      2288\n",
      "      editorial       0.45      0.34      0.38      1422\n",
      " belles_lettres       0.45      0.40      0.42      1186\n",
      "      adventure       0.52      0.52      0.52      1387\n",
      "        romance       0.63      0.17      0.27       515\n",
      "       religion       0.63      0.14      0.23       550\n",
      "science_fiction       0.41      0.46      0.43      1361\n",
      "          humor       0.83      0.04      0.07       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.55      0.37      0.38     17202\n",
      "   weighted avg       0.50      0.47      0.45     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 8.41602611541748]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 1, True, False] \n",
      " 0.5340076735263342 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.51      0.49      1449\n",
      "        mystery       0.48      0.59      0.53      2121\n",
      "     government       0.46      0.31      0.37       904\n",
      "        hobbies       0.47      0.44      0.45      1268\n",
      "           news       0.62      0.67      0.64       890\n",
      "        fiction       0.62      0.62      0.62      1281\n",
      "        reviews       0.57      0.24      0.34       314\n",
      "           lore       0.66      0.75      0.70      2288\n",
      "      editorial       0.48      0.45      0.47      1422\n",
      " belles_lettres       0.47      0.49      0.48      1186\n",
      "      adventure       0.57      0.56      0.57      1387\n",
      "        romance       0.55      0.43      0.48       515\n",
      "       religion       0.60      0.36      0.45       550\n",
      "science_fiction       0.45      0.50      0.47      1361\n",
      "          humor       0.60      0.32      0.41       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.54      0.48      0.50     17202\n",
      "   weighted avg       0.53      0.53      0.53     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 2.094285011291504]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 1, True, False] \n",
      " 0.14597139867457273 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.16      0.12      0.13      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      0.99      0.25      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.15     17202\n",
      "      macro avg       0.02      0.07      0.03     17202\n",
      "   weighted avg       0.04      0.15      0.05     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 6.407677173614502]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 1, True, False] \n",
      " 0.17951400999883735 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.15      0.47      0.23      1449\n",
      "        mystery       0.15      0.48      0.23      2121\n",
      "     government       1.00      0.01      0.02       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       1.00      0.00      0.01      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.22      0.55      0.32      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.24      0.08      0.12      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.18      0.11      0.06     17202\n",
      "   weighted avg       0.21      0.18      0.10     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.0500149726867676]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 1, True, False] \n",
      " 0.43169398907103823 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.46      0.41      0.43      1449\n",
      "        mystery       0.43      0.53      0.47      2121\n",
      "     government       0.27      0.24      0.25       904\n",
      "        hobbies       0.29      0.38      0.33      1268\n",
      "           news       0.45      0.55      0.50       890\n",
      "        fiction       0.54      0.51      0.53      1281\n",
      "        reviews       0.07      0.03      0.04       314\n",
      "           lore       0.64      0.65      0.64      2288\n",
      "      editorial       0.34      0.36      0.35      1422\n",
      " belles_lettres       0.46      0.41      0.43      1186\n",
      "      adventure       0.53      0.46      0.49      1387\n",
      "        romance       0.33      0.26      0.29       515\n",
      "       religion       0.23      0.09      0.12       550\n",
      "science_fiction       0.35      0.40      0.37      1361\n",
      "          humor       0.20      0.07      0.10       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.37      0.36      0.36     17202\n",
      "   weighted avg       0.43      0.43      0.43     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 30.148328065872192]\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 2, True, False] \n",
      " 0.48703639111731195 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.49      0.38      0.43      1449\n",
      "        mystery       0.45      0.55      0.50      2121\n",
      "     government       0.34      0.35      0.35       904\n",
      "        hobbies       0.40      0.40      0.40      1268\n",
      "           news       0.60      0.64      0.62       890\n",
      "        fiction       0.60      0.55      0.58      1281\n",
      "        reviews       0.26      0.26      0.26       314\n",
      "           lore       0.70      0.67      0.68      2288\n",
      "      editorial       0.42      0.44      0.43      1422\n",
      " belles_lettres       0.42      0.44      0.43      1186\n",
      "      adventure       0.53      0.55      0.54      1387\n",
      "        romance       0.51      0.33      0.41       515\n",
      "       religion       0.41      0.35      0.38       550\n",
      "science_fiction       0.42      0.42      0.42      1361\n",
      "          humor       0.27      0.37      0.31       266\n",
      "\n",
      "       accuracy                           0.49     17202\n",
      "      macro avg       0.46      0.45      0.45     17202\n",
      "   weighted avg       0.49      0.49      0.49     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.1430649757385254]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 2, True, False] \n",
      " 0.29258225787699105 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.52      0.23      0.32      1449\n",
      "        mystery       0.19      0.82      0.30      2121\n",
      "     government       1.00      0.01      0.01       904\n",
      "        hobbies       0.68      0.10      0.18      1268\n",
      "           news       0.85      0.08      0.15       890\n",
      "        fiction       0.90      0.09      0.17      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.35      0.83      0.50      2288\n",
      "      editorial       0.72      0.03      0.06      1422\n",
      " belles_lettres       0.72      0.09      0.15      1186\n",
      "      adventure       0.69      0.21      0.32      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.39      0.22      0.28      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.29     17202\n",
      "      macro avg       0.47      0.18      0.16     17202\n",
      "   weighted avg       0.52      0.29      0.23     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.46068882942199707]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 2, True, False] \n",
      " 0.46424834321590513 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.49      0.45      1449\n",
      "        mystery       0.37      0.62      0.47      2121\n",
      "     government       0.55      0.16      0.25       904\n",
      "        hobbies       0.43      0.36      0.39      1268\n",
      "           news       0.70      0.52      0.60       890\n",
      "        fiction       0.57      0.54      0.55      1281\n",
      "        reviews       1.00      0.03      0.05       314\n",
      "           lore       0.51      0.77      0.62      2288\n",
      "      editorial       0.46      0.31      0.37      1422\n",
      " belles_lettres       0.45      0.39      0.41      1186\n",
      "      adventure       0.50      0.55      0.52      1387\n",
      "        romance       0.76      0.16      0.26       515\n",
      "       religion       0.72      0.09      0.16       550\n",
      "science_fiction       0.40      0.46      0.42      1361\n",
      "          humor       0.88      0.03      0.05       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.58      0.36      0.37     17202\n",
      "   weighted avg       0.51      0.46      0.44     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 33.771337032318115]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 2, True, False] \n",
      " 0.5415068015347053 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.47      0.50      0.48      1449\n",
      "        mystery       0.49      0.60      0.54      2121\n",
      "     government       0.50      0.31      0.38       904\n",
      "        hobbies       0.46      0.45      0.46      1268\n",
      "           news       0.68      0.66      0.67       890\n",
      "        fiction       0.62      0.64      0.63      1281\n",
      "        reviews       0.64      0.25      0.35       314\n",
      "           lore       0.65      0.76      0.70      2288\n",
      "      editorial       0.50      0.46      0.48      1422\n",
      " belles_lettres       0.48      0.50      0.49      1186\n",
      "      adventure       0.60      0.61      0.60      1387\n",
      "        romance       0.58      0.41      0.48       515\n",
      "       religion       0.65      0.35      0.45       550\n",
      "science_fiction       0.43      0.48      0.45      1361\n",
      "          humor       0.61      0.32      0.42       266\n",
      "\n",
      "       accuracy                           0.54     17202\n",
      "      macro avg       0.56      0.49      0.51     17202\n",
      "   weighted avg       0.54      0.54      0.54     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 4.658125877380371]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 2, True, False] \n",
      " 0.14149517497965353 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.30      0.07      0.12      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.14      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.03      0.07      0.02     17202\n",
      "   weighted avg       0.05      0.14      0.05     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 10.032385110855103]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 2, True, False] \n",
      " 0.18462969422160214 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.16      0.49      0.24      1449\n",
      "        mystery       0.19      0.23      0.21      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.20      0.73      0.31      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.26      0.03      0.06      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.16      0.19      0.18      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.07      0.11      0.07     17202\n",
      "   weighted avg       0.10      0.18      0.11     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 4.906863689422607]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 2, True, False] \n",
      " 0.3856528310661551 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.36      0.39      1449\n",
      "        mystery       0.57      0.41      0.48      2121\n",
      "     government       0.18      0.20      0.19       904\n",
      "        hobbies       0.27      0.41      0.32      1268\n",
      "           news       0.42      0.57      0.48       890\n",
      "        fiction       0.50      0.48      0.49      1281\n",
      "        reviews       0.10      0.09      0.09       314\n",
      "           lore       0.65      0.65      0.65      2288\n",
      "      editorial       0.23      0.31      0.26      1422\n",
      " belles_lettres       0.39      0.36      0.37      1186\n",
      "      adventure       0.60      0.37      0.46      1387\n",
      "        romance       0.14      0.25      0.18       515\n",
      "       religion       0.13      0.02      0.04       550\n",
      "science_fiction       0.24      0.28      0.26      1361\n",
      "          humor       0.06      0.01      0.02       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.33      0.32      0.31     17202\n",
      "   weighted avg       0.41      0.39      0.39     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 120.75486612319946]\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Déjà vu\n",
      "['Perceptron', 1, 3, True, False] \n",
      " 0.48133937914196023 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.50      0.37      0.42      1449\n",
      "        mystery       0.44      0.55      0.49      2121\n",
      "     government       0.35      0.32      0.34       904\n",
      "        hobbies       0.38      0.38      0.38      1268\n",
      "           news       0.66      0.63      0.64       890\n",
      "        fiction       0.59      0.53      0.56      1281\n",
      "        reviews       0.24      0.25      0.24       314\n",
      "           lore       0.62      0.74      0.67      2288\n",
      "      editorial       0.46      0.39      0.43      1422\n",
      " belles_lettres       0.44      0.41      0.43      1186\n",
      "      adventure       0.53      0.53      0.53      1387\n",
      "        romance       0.41      0.43      0.42       515\n",
      "       religion       0.40      0.36      0.38       550\n",
      "science_fiction       0.37      0.42      0.39      1361\n",
      "          humor       0.45      0.23      0.30       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.46      0.43      0.44     17202\n",
      "   weighted avg       0.48      0.48      0.48     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 1.757939338684082]\n",
      "  Déjà vu\n",
      "['MultinomialNB', 1, 3, True, False] \n",
      " 0.2736891059179165 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.18      0.26      1449\n",
      "        mystery       0.18      0.80      0.29      2121\n",
      "     government       1.00      0.01      0.02       904\n",
      "        hobbies       0.63      0.07      0.13      1268\n",
      "           news       0.84      0.09      0.16       890\n",
      "        fiction       0.90      0.07      0.13      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.34      0.84      0.48      2288\n",
      "      editorial       0.71      0.02      0.03      1422\n",
      " belles_lettres       0.58      0.05      0.10      1186\n",
      "      adventure       0.72      0.16      0.25      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.39      0.19      0.25      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.27     17202\n",
      "      macro avg       0.45      0.17      0.14     17202\n",
      "   weighted avg       0.51      0.27      0.20     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 0.7163639068603516]\n",
      "  Déjà vu\n",
      "['Logistic Regression', 1, 3, True, False] \n",
      " 0.4321590512731078 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.39      0.46      0.43      1449\n",
      "        mystery       0.34      0.63      0.44      2121\n",
      "     government       0.54      0.10      0.17       904\n",
      "        hobbies       0.39      0.32      0.35      1268\n",
      "           news       0.71      0.47      0.57       890\n",
      "        fiction       0.53      0.48      0.50      1281\n",
      "        reviews       1.00      0.01      0.02       314\n",
      "           lore       0.47      0.77      0.58      2288\n",
      "      editorial       0.46      0.24      0.32      1422\n",
      " belles_lettres       0.46      0.34      0.39      1186\n",
      "      adventure       0.48      0.52      0.50      1387\n",
      "        romance       0.82      0.11      0.19       515\n",
      "       religion       0.76      0.03      0.07       550\n",
      "science_fiction       0.37      0.43      0.39      1361\n",
      "          humor       0.67      0.01      0.01       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.56      0.33      0.33     17202\n",
      "   weighted avg       0.49      0.43      0.40     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 106.4325602054596]\n",
      "  Déjà vu\n",
      "['linear_svc', 1, 3, True, False] \n",
      " 0.5282525287757237 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.45      0.49      0.47      1449\n",
      "        mystery       0.46      0.61      0.52      2121\n",
      "     government       0.50      0.29      0.37       904\n",
      "        hobbies       0.45      0.43      0.44      1268\n",
      "           news       0.70      0.66      0.68       890\n",
      "        fiction       0.62      0.62      0.62      1281\n",
      "        reviews       0.71      0.18      0.29       314\n",
      "           lore       0.62      0.77      0.69      2288\n",
      "      editorial       0.50      0.43      0.46      1422\n",
      " belles_lettres       0.48      0.48      0.48      1186\n",
      "      adventure       0.58      0.60      0.59      1387\n",
      "        romance       0.64      0.36      0.46       515\n",
      "       religion       0.69      0.30      0.42       550\n",
      "science_fiction       0.41      0.47      0.44      1361\n",
      "          humor       0.59      0.26      0.36       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.56      0.46      0.49     17202\n",
      "   weighted avg       0.54      0.53      0.52     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 9.031398057937622]\n",
      "  Déjà vu\n",
      "['Random Forest', 1, 3, True, False] \n",
      " 0.13696081850947564 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.61      0.03      0.06      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.05      0.07      0.02     17202\n",
      "   weighted avg       0.09      0.14      0.04     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 18.054501056671143]\n",
      "  Déjà vu\n",
      "['Decision Tree', 1, 3, True, False] \n",
      " 0.17695616788745494 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.19      0.05      0.08      1449\n",
      "        mystery       0.17      0.11      0.13      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.43      0.03      0.06       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.18      0.78      0.30      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.28      0.09      0.14      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.15      0.59      0.25      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.18     17202\n",
      "      macro avg       0.09      0.11      0.06     17202\n",
      "   weighted avg       0.12      0.18      0.10     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 18.42498207092285]\n",
      "  Déjà vu\n",
      "['Perceptron multicouche', 1, 3, True, False] \n",
      " 0.36356237646785255 \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.27      0.50      0.35      1449\n",
      "        mystery       0.54      0.35      0.42      2121\n",
      "     government       0.18      0.20      0.19       904\n",
      "        hobbies       0.35      0.26      0.30      1268\n",
      "           news       0.59      0.53      0.56       890\n",
      "        fiction       0.84      0.27      0.41      1281\n",
      "        reviews       0.06      0.05      0.06       314\n",
      "           lore       0.67      0.62      0.64      2288\n",
      "      editorial       0.20      0.42      0.27      1422\n",
      " belles_lettres       0.36      0.29      0.32      1186\n",
      "      adventure       0.65      0.33      0.44      1387\n",
      "        romance       0.13      0.27      0.18       515\n",
      "       religion       0.11      0.18      0.14       550\n",
      "science_fiction       0.44      0.28      0.34      1361\n",
      "          humor       0.06      0.04      0.05       266\n",
      "\n",
      "       accuracy                           0.36     17202\n",
      "      macro avg       0.36      0.31      0.31     17202\n",
      "   weighted avg       0.44      0.36      0.38     17202\n",
      " \n",
      " ['Temps_execution(secondes)', 543.877035856247]\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),max_df = 0.7,lowercase = False)\n",
    "    X = V.fit_transform(brown['raw_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            \n",
    "            print(report)\n",
    "            #print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5415068015347053, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.5340076735263342, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.5282525287757237, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.48703639111731195, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.48133937914196023, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.4688408324613417, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.46424834321590513, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.46407394489012904, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.4321590512731078, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.43169398907103823, \"['Perceptron multicouche', 1, 1, True, False]\"]\n",
      "[0.3856528310661551, \"['Perceptron multicouche', 1, 2, True, False]\"]\n",
      "[0.36356237646785255, \"['Perceptron multicouche', 1, 3, True, False]\"]\n",
      "[0.3203115916753866, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.29258225787699105, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.2736891059179165, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.18462969422160214, \"['Decision Tree', 1, 2, True, False]\"]\n",
      "[0.17951400999883735, \"['Decision Tree', 1, 1, True, False]\"]\n",
      "[0.17695616788745494, \"['Decision Tree', 1, 3, True, False]\"]\n",
      "[0.14597139867457273, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.14149517497965353, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.13696081850947564, \"['Random Forest', 1, 3, True, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5415068015347053, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.5413905359841878, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.5412161376584118, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.5411580048831531, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.535344727357284, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.5340076735263342, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.5331356818974538, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.5330775491221951, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.529880246482967, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.5297639809324497, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.5286013254272759, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.5282525287757237, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.48703639111731195, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.4856412045111034, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.4842460179048948, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.48378095570282525, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.48133937914196023, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.48069991861411465, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.4793047320079061, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.47552610161609116, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.47058481571910243, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.46918962911289386, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.4688408324613417, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.46750377863039183, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.4658760609231485, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.46558539704685503, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.4644808743169399, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.46424834321590513, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.46407394489012904, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.4629694221602139, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.4611091733519358, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.45907452621788164, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.43680967329380305, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.432682246250436, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.4321590512731078, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.43169398907103823, \"['Perceptron multicouche', 1, 1, True, False]\"]\n",
      "[0.4296593419369841, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.42285780723171723, \"['Perceptron multicouche', 1, 1, True, True]\"]\n",
      "[0.4180327868852459, \"['Perceptron multicouche', 1, 1, False, True]\"]\n",
      "[0.41721892803162425, \"['Perceptron multicouche', 1, 1, False, False]\"]\n",
      "[0.4040227880479014, \"['Perceptron multicouche', 1, 2, False, True]\"]\n",
      "[0.3874549470991745, \"['Perceptron multicouche', 1, 3, False, False]\"]\n",
      "[0.3856528310661551, \"['Perceptron multicouche', 1, 2, True, False]\"]\n",
      "[0.3768166492268341, \"['Perceptron multicouche', 1, 3, True, True]\"]\n",
      "[0.37449133821648645, \"['Perceptron multicouche', 1, 2, False, False]\"]\n",
      "[0.3730380188350192, \"['Perceptron multicouche', 1, 3, False, True]\"]\n",
      "[0.36356237646785255, \"['Perceptron multicouche', 1, 3, True, False]\"]\n",
      "[0.35280781304499476, \"['Perceptron multicouche', 1, 2, True, True]\"]\n",
      "[0.3249040809208232, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.3203115916753866, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.3107196837577026, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.3057202650854552, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.2960120916172538, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.29258225787699105, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.28874549470991745, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.28618765259853507, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.27589815137774676, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.2736891059179165, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.270026741076619, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.26898035112196256, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.1926520172073015, \"['Decision Tree', 1, 2, False, False]\"]\n",
      "[0.19137309615161027, \"['Decision Tree', 1, 3, False, True]\"]\n",
      "[0.19090803394954076, \"['Decision Tree', 1, 2, False, True]\"]\n",
      "[0.18462969422160214, \"['Decision Tree', 1, 2, True, False]\"]\n",
      "[0.18218811766073711, \"['Decision Tree', 1, 1, False, False]\"]\n",
      "[0.18149052435763283, \"['Decision Tree', 1, 3, True, True]\"]\n",
      "[0.17951400999883735, \"['Decision Tree', 1, 1, True, False]\"]\n",
      "[0.1791652133472852, \"['Decision Tree', 1, 3, False, False]\"]\n",
      "[0.17782815951633532, \"['Decision Tree', 1, 1, False, True]\"]\n",
      "[0.1772468317637484, \"['Decision Tree', 1, 1, True, True]\"]\n",
      "[0.17695616788745494, \"['Decision Tree', 1, 3, True, False]\"]\n",
      "[0.17550284850598769, \"['Decision Tree', 1, 2, True, True]\"]\n",
      "[0.14597139867457273, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.143471689338449, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.14225090105801652, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.1417277060806883, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.14149517497965353, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.14085571445180792, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.13841413789094292, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.13765841181257993, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.13754214626206254, \"['Random Forest', 1, 3, False, True]\"]\n",
      "[0.1373096151610278, \"['Random Forest', 1, 3, True, True]\"]\n",
      "[0.13696081850947564, \"['Random Forest', 1, 3, True, False]\"]\n"
     ]
    }
   ],
   "source": [
    "for res in sorted(Resultat,reverse=True):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clacification des textes sans POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur : \n",
      "ngram_range (1, 1) :\n",
      "889937\n",
      "ngram_range (1, 2) :\n",
      "1812617\n",
      "ngram_range (1, 3) :\n",
      "2686375\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur : \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(brown['tokenized_text']).getnnz())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur sans stopwords: \n",
      "ngram_range (1, 1) :\n",
      "512062\n",
      "ngram_range (1, 2) :\n",
      "978828\n",
      "ngram_range (1, 3) :\n",
      "1391398\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur sans stopwords: \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False,stop_words='english')\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(brown['tokenized_text']).getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"Brown_original.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.4488\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.48      0.43      1449\n",
      "        mystery       0.38      0.55      0.45      2121\n",
      "     government       0.36      0.28      0.32       904\n",
      "        hobbies       0.37      0.37      0.37      1268\n",
      "           news       0.51      0.54      0.52       890\n",
      "        fiction       0.53      0.52      0.52      1281\n",
      "        reviews       0.32      0.22      0.26       314\n",
      "           lore       0.67      0.62      0.64      2288\n",
      "      editorial       0.46      0.30      0.37      1422\n",
      " belles_lettres       0.46      0.38      0.42      1186\n",
      "      adventure       0.50      0.47      0.48      1387\n",
      "        romance       0.34      0.38      0.36       515\n",
      "       religion       0.43      0.31      0.36       550\n",
      "science_fiction       0.40      0.39      0.40      1361\n",
      "          humor       0.25      0.30      0.27       266\n",
      "\n",
      "       accuracy                           0.45     17202\n",
      "      macro avg       0.42      0.41      0.41     17202\n",
      "   weighted avg       0.46      0.45      0.45     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.6053318977355957)\n",
      "  MultinomialNB classifier : 0.3598\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.54      0.37      0.43      1449\n",
      "        mystery       0.22      0.81      0.34      2121\n",
      "     government       0.76      0.01      0.03       904\n",
      "        hobbies       0.63      0.20      0.30      1268\n",
      "           news       0.96      0.13      0.22       890\n",
      "        fiction       0.84      0.22      0.35      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.38      0.84      0.53      2288\n",
      "      editorial       0.62      0.15      0.25      1422\n",
      " belles_lettres       0.72      0.20      0.31      1186\n",
      "      adventure       0.68      0.34      0.46      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.43      0.31      0.36      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.36     17202\n",
      "      macro avg       0.52      0.24      0.24     17202\n",
      "   weighted avg       0.54      0.36      0.32     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.36618804931640625)\n",
      "  Logistic Regression classifier : 0.4794\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.45      0.47      0.46      1449\n",
      "        mystery       0.38      0.61      0.47      2121\n",
      "     government       0.48      0.19      0.27       904\n",
      "        hobbies       0.43      0.40      0.41      1268\n",
      "           news       0.71      0.50      0.59       890\n",
      "        fiction       0.59      0.57      0.58      1281\n",
      "        reviews       0.95      0.07      0.12       314\n",
      "           lore       0.52      0.78      0.63      2288\n",
      "      editorial       0.45      0.37      0.41      1422\n",
      " belles_lettres       0.46      0.42      0.44      1186\n",
      "      adventure       0.53      0.55      0.54      1387\n",
      "        romance       0.69      0.22      0.33       515\n",
      "       religion       0.69      0.15      0.25       550\n",
      "science_fiction       0.42      0.45      0.43      1361\n",
      "          humor       0.86      0.07      0.13       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.57      0.39      0.40     17202\n",
      "   weighted avg       0.51      0.48      0.46     17202\n",
      "\n",
      "('Temps_execution(secondes)', 7.687880039215088)\n",
      "  linear_svc classifier : 0.5265\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.49      0.48      1449\n",
      "        mystery       0.48      0.57      0.52      2121\n",
      "     government       0.46      0.33      0.38       904\n",
      "        hobbies       0.46      0.44      0.45      1268\n",
      "           news       0.58      0.67      0.62       890\n",
      "        fiction       0.61      0.61      0.61      1281\n",
      "        reviews       0.56      0.25      0.35       314\n",
      "           lore       0.66      0.75      0.70      2288\n",
      "      editorial       0.47      0.44      0.45      1422\n",
      " belles_lettres       0.45      0.48      0.47      1186\n",
      "      adventure       0.57      0.54      0.55      1387\n",
      "        romance       0.54      0.44      0.48       515\n",
      "       religion       0.60      0.36      0.45       550\n",
      "science_fiction       0.45      0.48      0.46      1361\n",
      "          humor       0.54      0.33      0.41       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.53      0.48      0.49     17202\n",
      "   weighted avg       0.53      0.53      0.52     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.9219820499420166)\n",
      "  Random Forest classifier : 0.1395\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.55      0.05      0.10      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.05      0.07      0.02     17202\n",
      "   weighted avg       0.09      0.14      0.04     17202\n",
      "\n",
      "('Temps_execution(secondes)', 6.226995944976807)\n",
      "  Decision Tree classifier : 0.1644\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.12      0.69      0.21      1449\n",
      "        mystery       0.20      0.05      0.08      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.20      0.69      0.30      2288\n",
      "      editorial       0.16      0.00      0.00      1422\n",
      " belles_lettres       0.50      0.00      0.01      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.27      0.10      0.15      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.10      0.10      0.05     17202\n",
      "   weighted avg       0.13      0.16      0.08     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.9759800434112549)\n",
      "  Perceptron multicouche classifier : 0.4408\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.43      0.43      1449\n",
      "        mystery       0.45      0.50      0.47      2121\n",
      "     government       0.27      0.25      0.26       904\n",
      "        hobbies       0.31      0.36      0.33      1268\n",
      "           news       0.58      0.55      0.56       890\n",
      "        fiction       0.58      0.56      0.57      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.67      0.67      0.67      2288\n",
      "      editorial       0.39      0.43      0.41      1422\n",
      " belles_lettres       0.41      0.40      0.40      1186\n",
      "      adventure       0.51      0.49      0.50      1387\n",
      "        romance       0.12      0.15      0.13       515\n",
      "       religion       0.30      0.15      0.20       550\n",
      "science_fiction       0.35      0.39      0.37      1361\n",
      "          humor       0.16      0.11      0.13       266\n",
      "\n",
      "       accuracy                           0.44     17202\n",
      "      macro avg       0.37      0.36      0.36     17202\n",
      "   weighted avg       0.43      0.44      0.44     17202\n",
      "\n",
      "('Temps_execution(secondes)', 27.458624124526978)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.4658\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.39      0.44      1449\n",
      "        mystery       0.42      0.55      0.48      2121\n",
      "     government       0.38      0.26      0.31       904\n",
      "        hobbies       0.39      0.35      0.37      1268\n",
      "           news       0.45      0.64      0.53       890\n",
      "        fiction       0.52      0.58      0.55      1281\n",
      "        reviews       0.28      0.20      0.23       314\n",
      "           lore       0.60      0.70      0.65      2288\n",
      "      editorial       0.46      0.36      0.40      1422\n",
      " belles_lettres       0.45      0.37      0.40      1186\n",
      "      adventure       0.50      0.54      0.52      1387\n",
      "        romance       0.38      0.42      0.40       515\n",
      "       religion       0.43      0.28      0.34       550\n",
      "science_fiction       0.40      0.34      0.37      1361\n",
      "          humor       0.28      0.28      0.28       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.43      0.42      0.42     17202\n",
      "   weighted avg       0.46      0.47      0.46     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.9825420379638672)\n",
      "  MultinomialNB classifier : 0.2853\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.55      0.21      0.30      1449\n",
      "        mystery       0.18      0.79      0.30      2121\n",
      "     government       1.00      0.01      0.03       904\n",
      "        hobbies       0.67      0.10      0.17      1268\n",
      "           news       0.98      0.06      0.12       890\n",
      "        fiction       0.92      0.11      0.19      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.33      0.86      0.48      2288\n",
      "      editorial       0.76      0.04      0.08      1422\n",
      " belles_lettres       0.70      0.08      0.15      1186\n",
      "      adventure       0.78      0.16      0.26      1387\n",
      "        romance       1.00      0.00      0.01       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.44      0.18      0.26      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.29     17202\n",
      "      macro avg       0.55      0.17      0.16     17202\n",
      "   weighted avg       0.57      0.29      0.22     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.5157938003540039)\n",
      "  Logistic Regression classifier : 0.4338\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.45      0.44      1449\n",
      "        mystery       0.33      0.62      0.43      2121\n",
      "     government       0.61      0.10      0.16       904\n",
      "        hobbies       0.40      0.34      0.36      1268\n",
      "           news       0.80      0.41      0.55       890\n",
      "        fiction       0.58      0.47      0.52      1281\n",
      "        reviews       1.00      0.01      0.02       314\n",
      "           lore       0.44      0.81      0.57      2288\n",
      "      editorial       0.47      0.27      0.34      1422\n",
      " belles_lettres       0.46      0.33      0.39      1186\n",
      "      adventure       0.52      0.52      0.52      1387\n",
      "        romance       0.77      0.10      0.18       515\n",
      "       religion       0.86      0.02      0.04       550\n",
      "science_fiction       0.38      0.43      0.40      1361\n",
      "          humor       1.00      0.01      0.02       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.60      0.33      0.33     17202\n",
      "   weighted avg       0.51      0.43      0.40     17202\n",
      "\n",
      "('Temps_execution(secondes)', 51.93084001541138)\n",
      "  linear_svc classifier : 0.5227\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.49      0.49      0.49      1449\n",
      "        mystery       0.45      0.60      0.52      2121\n",
      "     government       0.52      0.28      0.36       904\n",
      "        hobbies       0.42      0.43      0.42      1268\n",
      "           news       0.70      0.62      0.66       890\n",
      "        fiction       0.63      0.62      0.63      1281\n",
      "        reviews       0.63      0.18      0.28       314\n",
      "           lore       0.59      0.80      0.68      2288\n",
      "      editorial       0.49      0.43      0.46      1422\n",
      " belles_lettres       0.46      0.46      0.46      1186\n",
      "      adventure       0.58      0.58      0.58      1387\n",
      "        romance       0.62      0.37      0.46       515\n",
      "       religion       0.67      0.30      0.42       550\n",
      "science_fiction       0.41      0.44      0.43      1361\n",
      "          humor       0.60      0.26      0.36       266\n",
      "\n",
      "       accuracy                           0.52     17202\n",
      "      macro avg       0.55      0.46      0.48     17202\n",
      "   weighted avg       0.53      0.52      0.51     17202\n",
      "\n",
      "('Temps_execution(secondes)', 4.233221054077148)\n",
      "  Random Forest classifier : 0.1346\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.88      0.01      0.03      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.13     17202\n",
      "      macro avg       0.07      0.07      0.02     17202\n",
      "   weighted avg       0.13      0.13      0.03     17202\n",
      "\n",
      "('Temps_execution(secondes)', 13.228964805603027)\n",
      "  Decision Tree classifier : 0.1466\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.12      0.75      0.20      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.26      0.30      0.28      2288\n",
      "      editorial       0.25      0.00      0.01      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.20      0.07      0.10      1387\n",
      "        romance       0.56      0.03      0.06       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.23      0.09      0.13      1361\n",
      "          humor       1.00      0.02      0.03       266\n",
      "\n",
      "       accuracy                           0.15     17202\n",
      "      macro avg       0.24      0.08      0.05     17202\n",
      "   weighted avg       0.21      0.15      0.08     17202\n",
      "\n",
      "('Temps_execution(secondes)', 9.515714168548584)\n",
      "  Perceptron multicouche classifier : 0.3855\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.50      0.31      0.39      1449\n",
      "        mystery       0.56      0.35      0.43      2121\n",
      "     government       0.34      0.22      0.26       904\n",
      "        hobbies       0.31      0.34      0.33      1268\n",
      "           news       0.39      0.57      0.46       890\n",
      "        fiction       0.50      0.44      0.47      1281\n",
      "        reviews       0.05      0.11      0.06       314\n",
      "           lore       0.60      0.65      0.63      2288\n",
      "      editorial       0.46      0.29      0.36      1422\n",
      " belles_lettres       0.35      0.30      0.32      1186\n",
      "      adventure       0.52      0.46      0.49      1387\n",
      "        romance       0.33      0.26      0.29       515\n",
      "       religion       0.15      0.24      0.19       550\n",
      "science_fiction       0.33      0.35      0.34      1361\n",
      "          humor       0.04      0.24      0.07       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.36      0.34      0.34     17202\n",
      "   weighted avg       0.44      0.39      0.40     17202\n",
      "\n",
      "('Temps_execution(secondes)', 181.51245880126953)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.4573\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.44      0.43      1449\n",
      "        mystery       0.50      0.48      0.49      2121\n",
      "     government       0.29      0.30      0.30       904\n",
      "        hobbies       0.42      0.32      0.36      1268\n",
      "           news       0.42      0.65      0.51       890\n",
      "        fiction       0.48      0.60      0.54      1281\n",
      "        reviews       0.17      0.27      0.21       314\n",
      "           lore       0.64      0.68      0.66      2288\n",
      "      editorial       0.46      0.39      0.42      1422\n",
      " belles_lettres       0.52      0.30      0.38      1186\n",
      "      adventure       0.53      0.52      0.53      1387\n",
      "        romance       0.26      0.49      0.34       515\n",
      "       religion       0.38      0.30      0.34       550\n",
      "science_fiction       0.46      0.30      0.37      1361\n",
      "          humor       0.27      0.28      0.27       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.41      0.42      0.41     17202\n",
      "   weighted avg       0.47      0.46      0.45     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.5204291343688965)\n",
      "  MultinomialNB classifier : 0.2586\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.57      0.14      0.23      1449\n",
      "        mystery       0.17      0.77      0.28      2121\n",
      "     government       1.00      0.01      0.03       904\n",
      "        hobbies       0.74      0.06      0.11      1268\n",
      "           news       1.00      0.05      0.10       890\n",
      "        fiction       0.95      0.07      0.13      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.31      0.87      0.45      2288\n",
      "      editorial       0.73      0.02      0.04      1422\n",
      " belles_lettres       0.64      0.05      0.08      1186\n",
      "      adventure       0.83      0.09      0.17      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.45      0.13      0.20      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.26     17202\n",
      "      macro avg       0.56      0.15      0.12     17202\n",
      "   weighted avg       0.58      0.26      0.18     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.8725490570068359)\n",
      "  Logistic Regression classifier : 0.3951\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.42      0.41      1449\n",
      "        mystery       0.29      0.63      0.39      2121\n",
      "     government       0.70      0.04      0.08       904\n",
      "        hobbies       0.38      0.28      0.32      1268\n",
      "           news       0.87      0.34      0.49       890\n",
      "        fiction       0.62      0.38      0.47      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.38      0.83      0.52      2288\n",
      "      editorial       0.47      0.17      0.25      1422\n",
      " belles_lettres       0.49      0.28      0.35      1186\n",
      "      adventure       0.51      0.46      0.48      1387\n",
      "        romance       0.86      0.06      0.11       515\n",
      "       religion       1.00      0.01      0.01       550\n",
      "science_fiction       0.35      0.40      0.38      1361\n",
      "          humor       1.00      0.01      0.01       266\n",
      "\n",
      "       accuracy                           0.40     17202\n",
      "      macro avg       0.56      0.29      0.29     17202\n",
      "   weighted avg       0.49      0.40      0.36     17202\n",
      "\n",
      "('Temps_execution(secondes)', 163.66267108917236)\n",
      "  linear_svc classifier : 0.5090\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.50      0.49      1449\n",
      "        mystery       0.43      0.62      0.50      2121\n",
      "     government       0.54      0.23      0.32       904\n",
      "        hobbies       0.43      0.40      0.42      1268\n",
      "           news       0.70      0.59      0.64       890\n",
      "        fiction       0.64      0.60      0.62      1281\n",
      "        reviews       0.69      0.14      0.23       314\n",
      "           lore       0.55      0.80      0.65      2288\n",
      "      editorial       0.50      0.39      0.44      1422\n",
      " belles_lettres       0.45      0.43      0.44      1186\n",
      "      adventure       0.57      0.58      0.58      1387\n",
      "        romance       0.67      0.32      0.43       515\n",
      "       religion       0.73      0.23      0.35       550\n",
      "science_fiction       0.41      0.45      0.43      1361\n",
      "          humor       0.64      0.22      0.33       266\n",
      "\n",
      "       accuracy                           0.51     17202\n",
      "      macro avg       0.56      0.43      0.46     17202\n",
      "   weighted avg       0.53      0.51      0.50     17202\n",
      "\n",
      "('Temps_execution(secondes)', 7.613799810409546)\n",
      "  Random Forest classifier : 0.1345\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.87      0.01      0.02      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.13     17202\n",
      "      macro avg       0.07      0.07      0.02     17202\n",
      "   weighted avg       0.12      0.13      0.03     17202\n",
      "\n",
      "('Temps_execution(secondes)', 20.662521839141846)\n",
      "  Decision Tree classifier : 0.1502\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.12      0.62      0.20      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.63      0.01      0.03       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.20      0.48      0.28      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.29      0.02      0.03      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.26      0.10      0.15      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.15     17202\n",
      "      macro avg       0.17      0.08      0.05     17202\n",
      "   weighted avg       0.19      0.15      0.08     17202\n",
      "\n",
      "('Temps_execution(secondes)', 26.748650074005127)\n",
      "  Perceptron multicouche classifier : 0.3885\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.53      0.27      0.35      1449\n",
      "        mystery       0.54      0.28      0.37      2121\n",
      "     government       0.54      0.12      0.19       904\n",
      "        hobbies       0.30      0.31      0.31      1268\n",
      "           news       0.54      0.55      0.55       890\n",
      "        fiction       0.60      0.44      0.51      1281\n",
      "        reviews       0.35      0.07      0.12       314\n",
      "           lore       0.50      0.71      0.59      2288\n",
      "      editorial       0.43      0.27      0.33      1422\n",
      " belles_lettres       0.28      0.44      0.34      1186\n",
      "      adventure       0.48      0.44      0.46      1387\n",
      "        romance       0.15      0.49      0.23       515\n",
      "       religion       0.55      0.08      0.13       550\n",
      "science_fiction       0.23      0.49      0.32      1361\n",
      "          humor       0.32      0.12      0.17       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.42      0.34      0.33     17202\n",
      "   weighted avg       0.45      0.39      0.38     17202\n",
      "\n",
      "('Temps_execution(secondes)', 854.3497688770294)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N))\n",
    "    X = V.fit_transform(brown['tokenized_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5264504127427043, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, False, False]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, False, False]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, False, False]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.16439948843157773, \"['Decision Tree', 1, 1, False, False]\"]\n",
      "[0.15021509126845717, \"['Decision Tree', 1, 3, False, False]\"]\n",
      "[0.14661085920241831, \"['Decision Tree', 1, 2, False, False]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, False, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"Brown_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.4488\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.48      0.43      1449\n",
      "        mystery       0.38      0.55      0.45      2121\n",
      "     government       0.36      0.28      0.32       904\n",
      "        hobbies       0.37      0.37      0.37      1268\n",
      "           news       0.51      0.54      0.52       890\n",
      "        fiction       0.53      0.52      0.52      1281\n",
      "        reviews       0.32      0.22      0.26       314\n",
      "           lore       0.67      0.62      0.64      2288\n",
      "      editorial       0.46      0.30      0.37      1422\n",
      " belles_lettres       0.46      0.38      0.42      1186\n",
      "      adventure       0.50      0.47      0.48      1387\n",
      "        romance       0.34      0.38      0.36       515\n",
      "       religion       0.43      0.31      0.36       550\n",
      "science_fiction       0.40      0.39      0.40      1361\n",
      "          humor       0.25      0.30      0.27       266\n",
      "\n",
      "       accuracy                           0.45     17202\n",
      "      macro avg       0.42      0.41      0.41     17202\n",
      "   weighted avg       0.46      0.45      0.45     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.6013538837432861)\n",
      "  MultinomialNB classifier : 0.3598\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.54      0.37      0.43      1449\n",
      "        mystery       0.22      0.81      0.34      2121\n",
      "     government       0.76      0.01      0.03       904\n",
      "        hobbies       0.63      0.20      0.30      1268\n",
      "           news       0.96      0.13      0.22       890\n",
      "        fiction       0.84      0.22      0.35      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.38      0.84      0.53      2288\n",
      "      editorial       0.62      0.15      0.25      1422\n",
      " belles_lettres       0.72      0.20      0.31      1186\n",
      "      adventure       0.68      0.34      0.46      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.43      0.31      0.36      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.36     17202\n",
      "      macro avg       0.52      0.24      0.24     17202\n",
      "   weighted avg       0.54      0.36      0.32     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.3787980079650879)\n",
      "  Logistic Regression classifier : 0.4794\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.45      0.47      0.46      1449\n",
      "        mystery       0.38      0.61      0.47      2121\n",
      "     government       0.48      0.19      0.27       904\n",
      "        hobbies       0.43      0.40      0.41      1268\n",
      "           news       0.71      0.50      0.59       890\n",
      "        fiction       0.59      0.57      0.58      1281\n",
      "        reviews       0.95      0.07      0.12       314\n",
      "           lore       0.52      0.78      0.63      2288\n",
      "      editorial       0.45      0.37      0.41      1422\n",
      " belles_lettres       0.46      0.42      0.44      1186\n",
      "      adventure       0.53      0.55      0.54      1387\n",
      "        romance       0.69      0.22      0.33       515\n",
      "       religion       0.69      0.15      0.25       550\n",
      "science_fiction       0.42      0.45      0.43      1361\n",
      "          humor       0.86      0.07      0.13       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.57      0.39      0.40     17202\n",
      "   weighted avg       0.51      0.48      0.46     17202\n",
      "\n",
      "('Temps_execution(secondes)', 7.651608943939209)\n",
      "  linear_svc classifier : 0.5265\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.49      0.48      1449\n",
      "        mystery       0.48      0.57      0.52      2121\n",
      "     government       0.46      0.33      0.38       904\n",
      "        hobbies       0.46      0.44      0.45      1268\n",
      "           news       0.58      0.67      0.62       890\n",
      "        fiction       0.61      0.61      0.61      1281\n",
      "        reviews       0.56      0.25      0.35       314\n",
      "           lore       0.66      0.75      0.70      2288\n",
      "      editorial       0.47      0.44      0.45      1422\n",
      " belles_lettres       0.45      0.48      0.47      1186\n",
      "      adventure       0.57      0.54      0.55      1387\n",
      "        romance       0.54      0.44      0.48       515\n",
      "       religion       0.60      0.36      0.45       550\n",
      "science_fiction       0.45      0.48      0.46      1361\n",
      "          humor       0.54      0.33      0.41       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.53      0.48      0.49     17202\n",
      "   weighted avg       0.53      0.53      0.52     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.605010747909546)\n",
      "  Random Forest classifier : 0.1395\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.55      0.05      0.10      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.05      0.07      0.02     17202\n",
      "   weighted avg       0.09      0.14      0.04     17202\n",
      "\n",
      "('Temps_execution(secondes)', 6.114657163619995)\n",
      "  Decision Tree classifier : 0.1627\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.12      0.70      0.20      1449\n",
      "        mystery       0.21      0.07      0.10      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       1.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.20      0.64      0.30      2288\n",
      "      editorial       0.50      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.30      0.01      0.02      1387\n",
      "        romance       0.68      0.03      0.05       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.27      0.11      0.15      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.22      0.10      0.06     17202\n",
      "   weighted avg       0.24      0.16      0.09     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.9536502361297607)\n",
      "  Perceptron multicouche classifier : 0.4408\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.43      0.43      1449\n",
      "        mystery       0.45      0.50      0.47      2121\n",
      "     government       0.27      0.25      0.26       904\n",
      "        hobbies       0.31      0.36      0.33      1268\n",
      "           news       0.58      0.55      0.56       890\n",
      "        fiction       0.58      0.56      0.57      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.67      0.67      0.67      2288\n",
      "      editorial       0.39      0.43      0.41      1422\n",
      " belles_lettres       0.41      0.40      0.40      1186\n",
      "      adventure       0.51      0.49      0.50      1387\n",
      "        romance       0.12      0.15      0.13       515\n",
      "       religion       0.30      0.15      0.20       550\n",
      "science_fiction       0.35      0.39      0.37      1361\n",
      "          humor       0.16      0.11      0.13       266\n",
      "\n",
      "       accuracy                           0.44     17202\n",
      "      macro avg       0.37      0.36      0.36     17202\n",
      "   weighted avg       0.43      0.44      0.44     17202\n",
      "\n",
      "('Temps_execution(secondes)', 27.34295105934143)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.4658\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.39      0.44      1449\n",
      "        mystery       0.42      0.55      0.48      2121\n",
      "     government       0.38      0.26      0.31       904\n",
      "        hobbies       0.39      0.35      0.37      1268\n",
      "           news       0.45      0.64      0.53       890\n",
      "        fiction       0.52      0.58      0.55      1281\n",
      "        reviews       0.28      0.20      0.23       314\n",
      "           lore       0.60      0.70      0.65      2288\n",
      "      editorial       0.46      0.36      0.40      1422\n",
      " belles_lettres       0.45      0.37      0.40      1186\n",
      "      adventure       0.50      0.54      0.52      1387\n",
      "        romance       0.38      0.42      0.40       515\n",
      "       religion       0.43      0.28      0.34       550\n",
      "science_fiction       0.40      0.34      0.37      1361\n",
      "          humor       0.28      0.28      0.28       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.43      0.42      0.42     17202\n",
      "   weighted avg       0.46      0.47      0.46     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.00705885887146)\n",
      "  MultinomialNB classifier : 0.2853\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.55      0.21      0.30      1449\n",
      "        mystery       0.18      0.79      0.30      2121\n",
      "     government       1.00      0.01      0.03       904\n",
      "        hobbies       0.67      0.10      0.17      1268\n",
      "           news       0.98      0.06      0.12       890\n",
      "        fiction       0.92      0.11      0.19      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.33      0.86      0.48      2288\n",
      "      editorial       0.76      0.04      0.08      1422\n",
      " belles_lettres       0.70      0.08      0.15      1186\n",
      "      adventure       0.78      0.16      0.26      1387\n",
      "        romance       1.00      0.00      0.01       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.44      0.18      0.26      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.29     17202\n",
      "      macro avg       0.55      0.17      0.16     17202\n",
      "   weighted avg       0.57      0.29      0.22     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.5178160667419434)\n",
      "  Logistic Regression classifier : 0.4338\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.45      0.44      1449\n",
      "        mystery       0.33      0.62      0.43      2121\n",
      "     government       0.61      0.10      0.16       904\n",
      "        hobbies       0.40      0.34      0.36      1268\n",
      "           news       0.80      0.41      0.55       890\n",
      "        fiction       0.58      0.47      0.52      1281\n",
      "        reviews       1.00      0.01      0.02       314\n",
      "           lore       0.44      0.81      0.57      2288\n",
      "      editorial       0.47      0.27      0.34      1422\n",
      " belles_lettres       0.46      0.33      0.39      1186\n",
      "      adventure       0.52      0.52      0.52      1387\n",
      "        romance       0.77      0.10      0.18       515\n",
      "       religion       0.86      0.02      0.04       550\n",
      "science_fiction       0.38      0.43      0.40      1361\n",
      "          humor       1.00      0.01      0.02       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.60      0.33      0.33     17202\n",
      "   weighted avg       0.51      0.43      0.40     17202\n",
      "\n",
      "('Temps_execution(secondes)', 53.71195983886719)\n",
      "  linear_svc classifier : 0.5227\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.49      0.49      0.49      1449\n",
      "        mystery       0.45      0.60      0.52      2121\n",
      "     government       0.52      0.28      0.36       904\n",
      "        hobbies       0.42      0.43      0.42      1268\n",
      "           news       0.70      0.62      0.66       890\n",
      "        fiction       0.63      0.62      0.63      1281\n",
      "        reviews       0.63      0.18      0.28       314\n",
      "           lore       0.59      0.80      0.68      2288\n",
      "      editorial       0.49      0.43      0.46      1422\n",
      " belles_lettres       0.46      0.46      0.46      1186\n",
      "      adventure       0.58      0.58      0.58      1387\n",
      "        romance       0.62      0.37      0.46       515\n",
      "       religion       0.67      0.30      0.42       550\n",
      "science_fiction       0.41      0.44      0.43      1361\n",
      "          humor       0.60      0.26      0.36       266\n",
      "\n",
      "       accuracy                           0.52     17202\n",
      "      macro avg       0.55      0.46      0.48     17202\n",
      "   weighted avg       0.53      0.52      0.51     17202\n",
      "\n",
      "('Temps_execution(secondes)', 4.3521568775177)\n",
      "  Random Forest classifier : 0.1346\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.88      0.01      0.03      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.13     17202\n",
      "      macro avg       0.07      0.07      0.02     17202\n",
      "   weighted avg       0.13      0.13      0.03     17202\n",
      "\n",
      "('Temps_execution(secondes)', 13.309532880783081)\n",
      "  Decision Tree classifier : 0.1576\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.23      0.11      0.15      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.16      0.13      0.15      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       1.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.15      0.96      0.26      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.19      0.09      0.12      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.12      0.09      0.05     17202\n",
      "   weighted avg       0.15      0.16      0.07     17202\n",
      "\n",
      "('Temps_execution(secondes)', 9.67733097076416)\n",
      "  Perceptron multicouche classifier : 0.3855\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.50      0.31      0.39      1449\n",
      "        mystery       0.56      0.35      0.43      2121\n",
      "     government       0.34      0.22      0.26       904\n",
      "        hobbies       0.31      0.34      0.33      1268\n",
      "           news       0.39      0.57      0.46       890\n",
      "        fiction       0.50      0.44      0.47      1281\n",
      "        reviews       0.05      0.11      0.06       314\n",
      "           lore       0.60      0.65      0.63      2288\n",
      "      editorial       0.46      0.29      0.36      1422\n",
      " belles_lettres       0.35      0.30      0.32      1186\n",
      "      adventure       0.52      0.46      0.49      1387\n",
      "        romance       0.33      0.26      0.29       515\n",
      "       religion       0.15      0.24      0.19       550\n",
      "science_fiction       0.33      0.35      0.34      1361\n",
      "          humor       0.04      0.24      0.07       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.36      0.34      0.34     17202\n",
      "   weighted avg       0.44      0.39      0.40     17202\n",
      "\n",
      "('Temps_execution(secondes)', 181.0241141319275)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.4573\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.44      0.43      1449\n",
      "        mystery       0.50      0.48      0.49      2121\n",
      "     government       0.29      0.30      0.30       904\n",
      "        hobbies       0.42      0.32      0.36      1268\n",
      "           news       0.42      0.65      0.51       890\n",
      "        fiction       0.48      0.60      0.54      1281\n",
      "        reviews       0.17      0.27      0.21       314\n",
      "           lore       0.64      0.68      0.66      2288\n",
      "      editorial       0.46      0.39      0.42      1422\n",
      " belles_lettres       0.52      0.30      0.38      1186\n",
      "      adventure       0.53      0.52      0.53      1387\n",
      "        romance       0.26      0.49      0.34       515\n",
      "       religion       0.38      0.30      0.34       550\n",
      "science_fiction       0.46      0.30      0.37      1361\n",
      "          humor       0.27      0.28      0.27       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.41      0.42      0.41     17202\n",
      "   weighted avg       0.47      0.46      0.45     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.5669939517974854)\n",
      "  MultinomialNB classifier : 0.2586\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.57      0.14      0.23      1449\n",
      "        mystery       0.17      0.77      0.28      2121\n",
      "     government       1.00      0.01      0.03       904\n",
      "        hobbies       0.74      0.06      0.11      1268\n",
      "           news       1.00      0.05      0.10       890\n",
      "        fiction       0.95      0.07      0.13      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.31      0.87      0.45      2288\n",
      "      editorial       0.73      0.02      0.04      1422\n",
      " belles_lettres       0.64      0.05      0.08      1186\n",
      "      adventure       0.83      0.09      0.17      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.45      0.13      0.20      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.26     17202\n",
      "      macro avg       0.56      0.15      0.12     17202\n",
      "   weighted avg       0.58      0.26      0.18     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.9114949703216553)\n",
      "  Logistic Regression classifier : 0.3951\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.42      0.41      1449\n",
      "        mystery       0.29      0.63      0.39      2121\n",
      "     government       0.70      0.04      0.08       904\n",
      "        hobbies       0.38      0.28      0.32      1268\n",
      "           news       0.87      0.34      0.49       890\n",
      "        fiction       0.62      0.38      0.47      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.38      0.83      0.52      2288\n",
      "      editorial       0.47      0.17      0.25      1422\n",
      " belles_lettres       0.49      0.28      0.35      1186\n",
      "      adventure       0.51      0.46      0.48      1387\n",
      "        romance       0.86      0.06      0.11       515\n",
      "       religion       1.00      0.01      0.01       550\n",
      "science_fiction       0.35      0.40      0.38      1361\n",
      "          humor       1.00      0.01      0.01       266\n",
      "\n",
      "       accuracy                           0.40     17202\n",
      "      macro avg       0.56      0.29      0.29     17202\n",
      "   weighted avg       0.49      0.40      0.36     17202\n",
      "\n",
      "('Temps_execution(secondes)', 164.09090399742126)\n",
      "  linear_svc classifier : 0.5090\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.50      0.49      1449\n",
      "        mystery       0.43      0.62      0.50      2121\n",
      "     government       0.54      0.23      0.32       904\n",
      "        hobbies       0.43      0.40      0.42      1268\n",
      "           news       0.70      0.59      0.64       890\n",
      "        fiction       0.64      0.60      0.62      1281\n",
      "        reviews       0.69      0.14      0.23       314\n",
      "           lore       0.55      0.80      0.65      2288\n",
      "      editorial       0.50      0.39      0.44      1422\n",
      " belles_lettres       0.45      0.43      0.44      1186\n",
      "      adventure       0.57      0.58      0.58      1387\n",
      "        romance       0.67      0.32      0.43       515\n",
      "       religion       0.73      0.23      0.35       550\n",
      "science_fiction       0.41      0.45      0.43      1361\n",
      "          humor       0.64      0.22      0.33       266\n",
      "\n",
      "       accuracy                           0.51     17202\n",
      "      macro avg       0.56      0.43      0.46     17202\n",
      "   weighted avg       0.53      0.51      0.50     17202\n",
      "\n",
      "('Temps_execution(secondes)', 7.769305944442749)\n",
      "  Random Forest classifier : 0.1345\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.87      0.01      0.02      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.13     17202\n",
      "      macro avg       0.07      0.07      0.02     17202\n",
      "   weighted avg       0.12      0.13      0.03     17202\n",
      "\n",
      "('Temps_execution(secondes)', 20.865504026412964)\n",
      "  Decision Tree classifier : 0.1553\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.13      0.66      0.21      1449\n",
      "        mystery       0.16      0.03      0.05      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.50      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.18      0.71      0.28      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.26      0.03      0.05      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.08      0.09      0.04     17202\n",
      "   weighted avg       0.11      0.16      0.07     17202\n",
      "\n",
      "('Temps_execution(secondes)', 26.896647214889526)\n",
      "  Perceptron multicouche classifier : 0.3885\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.53      0.27      0.35      1449\n",
      "        mystery       0.54      0.28      0.37      2121\n",
      "     government       0.54      0.12      0.19       904\n",
      "        hobbies       0.30      0.31      0.31      1268\n",
      "           news       0.54      0.55      0.55       890\n",
      "        fiction       0.60      0.44      0.51      1281\n",
      "        reviews       0.35      0.07      0.12       314\n",
      "           lore       0.50      0.71      0.59      2288\n",
      "      editorial       0.43      0.27      0.33      1422\n",
      " belles_lettres       0.28      0.44      0.34      1186\n",
      "      adventure       0.48      0.44      0.46      1387\n",
      "        romance       0.15      0.49      0.23       515\n",
      "       religion       0.55      0.08      0.13       550\n",
      "science_fiction       0.23      0.49      0.32      1361\n",
      "          humor       0.32      0.12      0.17       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.42      0.34      0.33     17202\n",
      "   weighted avg       0.45      0.39      0.38     17202\n",
      "\n",
      "('Temps_execution(secondes)', 698.248072385788)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),max_df = 0.7)\n",
    "    X = V.fit_transform(brown['tokenized_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5264504127427043, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, True, False]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, True, False]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, True, False]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.1627136379490757, \"['Decision Tree', 1, 1, True, False]\"]\n",
      "[0.15759795372631089, \"['Decision Tree', 1, 2, True, False]\"]\n",
      "[0.15533077549122196, \"['Decision Tree', 1, 3, True, False]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, True, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"Brown_lower.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.4488\n",
      "('Temps_execution(secondes)', 0.6205136775970459)\n",
      "  MultinomialNB classifier : 0.3598\n",
      "('Temps_execution(secondes)', 0.36931395530700684)\n",
      "  Logistic Regression classifier : 0.4794\n",
      "('Temps_execution(secondes)', 7.0326879024505615)\n",
      "  linear_svc classifier : 0.5265\n",
      "('Temps_execution(secondes)', 1.5326461791992188)\n",
      "  Random Forest classifier : 0.1395\n",
      "('Temps_execution(secondes)', 6.034715890884399)\n",
      "  Decision Tree classifier : 0.1456\n",
      "('Temps_execution(secondes)', 0.9306049346923828)\n",
      "  Perceptron multicouche classifier : 0.4408\n",
      "('Temps_execution(secondes)', 26.77574896812439)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.4658\n",
      "('Temps_execution(secondes)', 0.9400770664215088)\n",
      "  MultinomialNB classifier : 0.2853\n",
      "('Temps_execution(secondes)', 0.5081541538238525)\n",
      "  Logistic Regression classifier : 0.4338\n",
      "('Temps_execution(secondes)', 48.948009967803955)\n",
      "  linear_svc classifier : 0.5227\n",
      "('Temps_execution(secondes)', 3.9146270751953125)\n",
      "  Random Forest classifier : 0.1346\n",
      "('Temps_execution(secondes)', 13.240721940994263)\n",
      "  Decision Tree classifier : 0.1519\n",
      "('Temps_execution(secondes)', 9.745928764343262)\n",
      "  Perceptron multicouche classifier : 0.3855\n",
      "('Temps_execution(secondes)', 203.85547590255737)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.4573\n",
      "('Temps_execution(secondes)', 1.5450408458709717)\n",
      "  MultinomialNB classifier : 0.2586\n",
      "('Temps_execution(secondes)', 0.9212610721588135)\n",
      "  Logistic Regression classifier : 0.3951\n",
      "('Temps_execution(secondes)', 170.70126485824585)\n",
      "  linear_svc classifier : 0.5090\n",
      "('Temps_execution(secondes)', 7.718867778778076)\n",
      "  Random Forest classifier : 0.1345\n",
      "('Temps_execution(secondes)', 20.710027933120728)\n",
      "  Decision Tree classifier : 0.1505\n",
      "('Temps_execution(secondes)', 26.81016492843628)\n",
      "  Perceptron multicouche classifier : 0.3885\n",
      "('Temps_execution(secondes)', 1075.73801612854)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase=True)\n",
    "    X = V.fit_transform(brown['tokenized_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5264504127427043, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, False, True]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, False, True]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, False, True]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.1519009417509592, \"['Decision Tree', 1, 2, False, True]\"]\n",
      "[0.15050575514475062, \"['Decision Tree', 1, 3, False, True]\"]\n",
      "[0.14562260202302058, \"['Decision Tree', 1, 1, False, True]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, False, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 21\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"Brown_lower_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.4488\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.48      0.43      1449\n",
      "        mystery       0.38      0.55      0.45      2121\n",
      "     government       0.36      0.28      0.32       904\n",
      "        hobbies       0.37      0.37      0.37      1268\n",
      "           news       0.51      0.54      0.52       890\n",
      "        fiction       0.53      0.52      0.52      1281\n",
      "        reviews       0.32      0.22      0.26       314\n",
      "           lore       0.67      0.62      0.64      2288\n",
      "      editorial       0.46      0.30      0.37      1422\n",
      " belles_lettres       0.46      0.38      0.42      1186\n",
      "      adventure       0.50      0.47      0.48      1387\n",
      "        romance       0.34      0.38      0.36       515\n",
      "       religion       0.43      0.31      0.36       550\n",
      "science_fiction       0.40      0.39      0.40      1361\n",
      "          humor       0.25      0.30      0.27       266\n",
      "\n",
      "       accuracy                           0.45     17202\n",
      "      macro avg       0.42      0.41      0.41     17202\n",
      "   weighted avg       0.46      0.45      0.45     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.5848138332366943)\n",
      "  MultinomialNB classifier : 0.3598\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.54      0.37      0.43      1449\n",
      "        mystery       0.22      0.81      0.34      2121\n",
      "     government       0.76      0.01      0.03       904\n",
      "        hobbies       0.63      0.20      0.30      1268\n",
      "           news       0.96      0.13      0.22       890\n",
      "        fiction       0.84      0.22      0.35      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.38      0.84      0.53      2288\n",
      "      editorial       0.62      0.15      0.25      1422\n",
      " belles_lettres       0.72      0.20      0.31      1186\n",
      "      adventure       0.68      0.34      0.46      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.43      0.31      0.36      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.36     17202\n",
      "      macro avg       0.52      0.24      0.24     17202\n",
      "   weighted avg       0.54      0.36      0.32     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.3730177879333496)\n",
      "  Logistic Regression classifier : 0.4794\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.45      0.47      0.46      1449\n",
      "        mystery       0.38      0.61      0.47      2121\n",
      "     government       0.48      0.19      0.27       904\n",
      "        hobbies       0.43      0.40      0.41      1268\n",
      "           news       0.71      0.50      0.59       890\n",
      "        fiction       0.59      0.57      0.58      1281\n",
      "        reviews       0.95      0.07      0.12       314\n",
      "           lore       0.52      0.78      0.63      2288\n",
      "      editorial       0.45      0.37      0.41      1422\n",
      " belles_lettres       0.46      0.42      0.44      1186\n",
      "      adventure       0.53      0.55      0.54      1387\n",
      "        romance       0.69      0.22      0.33       515\n",
      "       religion       0.69      0.15      0.25       550\n",
      "science_fiction       0.42      0.45      0.43      1361\n",
      "          humor       0.86      0.07      0.13       266\n",
      "\n",
      "       accuracy                           0.48     17202\n",
      "      macro avg       0.57      0.39      0.40     17202\n",
      "   weighted avg       0.51      0.48      0.46     17202\n",
      "\n",
      "('Temps_execution(secondes)', 7.1533544063568115)\n",
      "  linear_svc classifier : 0.5265\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.49      0.48      1449\n",
      "        mystery       0.48      0.57      0.52      2121\n",
      "     government       0.46      0.33      0.38       904\n",
      "        hobbies       0.46      0.44      0.45      1268\n",
      "           news       0.58      0.67      0.62       890\n",
      "        fiction       0.61      0.61      0.61      1281\n",
      "        reviews       0.56      0.25      0.35       314\n",
      "           lore       0.66      0.75      0.70      2288\n",
      "      editorial       0.47      0.44      0.45      1422\n",
      " belles_lettres       0.45      0.48      0.47      1186\n",
      "      adventure       0.57      0.54      0.55      1387\n",
      "        romance       0.54      0.44      0.48       515\n",
      "       religion       0.60      0.36      0.45       550\n",
      "science_fiction       0.45      0.48      0.46      1361\n",
      "          humor       0.54      0.33      0.41       266\n",
      "\n",
      "       accuracy                           0.53     17202\n",
      "      macro avg       0.53      0.48      0.49     17202\n",
      "   weighted avg       0.53      0.53      0.52     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.5838558673858643)\n",
      "  Random Forest classifier : 0.1395\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.55      0.05      0.10      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.14     17202\n",
      "      macro avg       0.05      0.07      0.02     17202\n",
      "   weighted avg       0.09      0.14      0.04     17202\n",
      "\n",
      "('Temps_execution(secondes)', 6.104804039001465)\n",
      "  Decision Tree classifier : 0.1595\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.20      0.18      0.19      1449\n",
      "        mystery       0.15      0.03      0.05      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.83      0.00      0.01      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.17      0.77      0.28      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.58      0.02      0.04       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.12      0.47      0.19      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.14      0.10      0.05     17202\n",
      "   weighted avg       0.15      0.16      0.08     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.9405269622802734)\n",
      "  Perceptron multicouche classifier : 0.4408\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.43      0.43      1449\n",
      "        mystery       0.45      0.50      0.47      2121\n",
      "     government       0.27      0.25      0.26       904\n",
      "        hobbies       0.31      0.36      0.33      1268\n",
      "           news       0.58      0.55      0.56       890\n",
      "        fiction       0.58      0.56      0.57      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.67      0.67      0.67      2288\n",
      "      editorial       0.39      0.43      0.41      1422\n",
      " belles_lettres       0.41      0.40      0.40      1186\n",
      "      adventure       0.51      0.49      0.50      1387\n",
      "        romance       0.12      0.15      0.13       515\n",
      "       religion       0.30      0.15      0.20       550\n",
      "science_fiction       0.35      0.39      0.37      1361\n",
      "          humor       0.16      0.11      0.13       266\n",
      "\n",
      "       accuracy                           0.44     17202\n",
      "      macro avg       0.37      0.36      0.36     17202\n",
      "   weighted avg       0.43      0.44      0.44     17202\n",
      "\n",
      "('Temps_execution(secondes)', 25.750011920928955)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.4658\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.51      0.39      0.44      1449\n",
      "        mystery       0.42      0.55      0.48      2121\n",
      "     government       0.38      0.26      0.31       904\n",
      "        hobbies       0.39      0.35      0.37      1268\n",
      "           news       0.45      0.64      0.53       890\n",
      "        fiction       0.52      0.58      0.55      1281\n",
      "        reviews       0.28      0.20      0.23       314\n",
      "           lore       0.60      0.70      0.65      2288\n",
      "      editorial       0.46      0.36      0.40      1422\n",
      " belles_lettres       0.45      0.37      0.40      1186\n",
      "      adventure       0.50      0.54      0.52      1387\n",
      "        romance       0.38      0.42      0.40       515\n",
      "       religion       0.43      0.28      0.34       550\n",
      "science_fiction       0.40      0.34      0.37      1361\n",
      "          humor       0.28      0.28      0.28       266\n",
      "\n",
      "       accuracy                           0.47     17202\n",
      "      macro avg       0.43      0.42      0.42     17202\n",
      "   weighted avg       0.46      0.47      0.46     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.9858958721160889)\n",
      "  MultinomialNB classifier : 0.2853\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.55      0.21      0.30      1449\n",
      "        mystery       0.18      0.79      0.30      2121\n",
      "     government       1.00      0.01      0.03       904\n",
      "        hobbies       0.67      0.10      0.17      1268\n",
      "           news       0.98      0.06      0.12       890\n",
      "        fiction       0.92      0.11      0.19      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.33      0.86      0.48      2288\n",
      "      editorial       0.76      0.04      0.08      1422\n",
      " belles_lettres       0.70      0.08      0.15      1186\n",
      "      adventure       0.78      0.16      0.26      1387\n",
      "        romance       1.00      0.00      0.01       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.44      0.18      0.26      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.29     17202\n",
      "      macro avg       0.55      0.17      0.16     17202\n",
      "   weighted avg       0.57      0.29      0.22     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.5210380554199219)\n",
      "  Logistic Regression classifier : 0.4338\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.43      0.45      0.44      1449\n",
      "        mystery       0.33      0.62      0.43      2121\n",
      "     government       0.61      0.10      0.16       904\n",
      "        hobbies       0.40      0.34      0.36      1268\n",
      "           news       0.80      0.41      0.55       890\n",
      "        fiction       0.58      0.47      0.52      1281\n",
      "        reviews       1.00      0.01      0.02       314\n",
      "           lore       0.44      0.81      0.57      2288\n",
      "      editorial       0.47      0.27      0.34      1422\n",
      " belles_lettres       0.46      0.33      0.39      1186\n",
      "      adventure       0.52      0.52      0.52      1387\n",
      "        romance       0.77      0.10      0.18       515\n",
      "       religion       0.86      0.02      0.04       550\n",
      "science_fiction       0.38      0.43      0.40      1361\n",
      "          humor       1.00      0.01      0.02       266\n",
      "\n",
      "       accuracy                           0.43     17202\n",
      "      macro avg       0.60      0.33      0.33     17202\n",
      "   weighted avg       0.51      0.43      0.40     17202\n",
      "\n",
      "('Temps_execution(secondes)', 50.18707299232483)\n",
      "  linear_svc classifier : 0.5227\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.49      0.49      0.49      1449\n",
      "        mystery       0.45      0.60      0.52      2121\n",
      "     government       0.52      0.28      0.36       904\n",
      "        hobbies       0.42      0.43      0.42      1268\n",
      "           news       0.70      0.62      0.66       890\n",
      "        fiction       0.63      0.62      0.63      1281\n",
      "        reviews       0.63      0.18      0.28       314\n",
      "           lore       0.59      0.80      0.68      2288\n",
      "      editorial       0.49      0.43      0.46      1422\n",
      " belles_lettres       0.46      0.46      0.46      1186\n",
      "      adventure       0.58      0.58      0.58      1387\n",
      "        romance       0.62      0.37      0.46       515\n",
      "       religion       0.67      0.30      0.42       550\n",
      "science_fiction       0.41      0.44      0.43      1361\n",
      "          humor       0.60      0.26      0.36       266\n",
      "\n",
      "       accuracy                           0.52     17202\n",
      "      macro avg       0.55      0.46      0.48     17202\n",
      "   weighted avg       0.53      0.52      0.51     17202\n",
      "\n",
      "('Temps_execution(secondes)', 3.701533317565918)\n",
      "  Random Forest classifier : 0.1346\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.88      0.01      0.03      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.13     17202\n",
      "      macro avg       0.07      0.07      0.02     17202\n",
      "   weighted avg       0.13      0.13      0.03     17202\n",
      "\n",
      "('Temps_execution(secondes)', 13.35493016242981)\n",
      "  Decision Tree classifier : 0.1596\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.19      0.13      0.16      1449\n",
      "        mystery       0.20      0.08      0.11      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.14      0.07      0.09      1268\n",
      "           news       1.00      0.00      0.00       890\n",
      "        fiction       1.00      0.00      0.01      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.16      0.95      0.27      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       1.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.16      0.09      0.11      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.26      0.09      0.05     17202\n",
      "   weighted avg       0.28      0.16      0.08     17202\n",
      "\n",
      "('Temps_execution(secondes)', 9.746025085449219)\n",
      "  Perceptron multicouche classifier : 0.3855\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.50      0.31      0.39      1449\n",
      "        mystery       0.56      0.35      0.43      2121\n",
      "     government       0.34      0.22      0.26       904\n",
      "        hobbies       0.31      0.34      0.33      1268\n",
      "           news       0.39      0.57      0.46       890\n",
      "        fiction       0.50      0.44      0.47      1281\n",
      "        reviews       0.05      0.11      0.06       314\n",
      "           lore       0.60      0.65      0.63      2288\n",
      "      editorial       0.46      0.29      0.36      1422\n",
      " belles_lettres       0.35      0.30      0.32      1186\n",
      "      adventure       0.52      0.46      0.49      1387\n",
      "        romance       0.33      0.26      0.29       515\n",
      "       religion       0.15      0.24      0.19       550\n",
      "science_fiction       0.33      0.35      0.34      1361\n",
      "          humor       0.04      0.24      0.07       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.36      0.34      0.34     17202\n",
      "   weighted avg       0.44      0.39      0.40     17202\n",
      "\n",
      "('Temps_execution(secondes)', 175.67560815811157)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.4573\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.42      0.44      0.43      1449\n",
      "        mystery       0.50      0.48      0.49      2121\n",
      "     government       0.29      0.30      0.30       904\n",
      "        hobbies       0.42      0.32      0.36      1268\n",
      "           news       0.42      0.65      0.51       890\n",
      "        fiction       0.48      0.60      0.54      1281\n",
      "        reviews       0.17      0.27      0.21       314\n",
      "           lore       0.64      0.68      0.66      2288\n",
      "      editorial       0.46      0.39      0.42      1422\n",
      " belles_lettres       0.52      0.30      0.38      1186\n",
      "      adventure       0.53      0.52      0.53      1387\n",
      "        romance       0.26      0.49      0.34       515\n",
      "       religion       0.38      0.30      0.34       550\n",
      "science_fiction       0.46      0.30      0.37      1361\n",
      "          humor       0.27      0.28      0.27       266\n",
      "\n",
      "       accuracy                           0.46     17202\n",
      "      macro avg       0.41      0.42      0.41     17202\n",
      "   weighted avg       0.47      0.46      0.45     17202\n",
      "\n",
      "('Temps_execution(secondes)', 1.4353370666503906)\n",
      "  MultinomialNB classifier : 0.2586\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.57      0.14      0.23      1449\n",
      "        mystery       0.17      0.77      0.28      2121\n",
      "     government       1.00      0.01      0.03       904\n",
      "        hobbies       0.74      0.06      0.11      1268\n",
      "           news       1.00      0.05      0.10       890\n",
      "        fiction       0.95      0.07      0.13      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.31      0.87      0.45      2288\n",
      "      editorial       0.73      0.02      0.04      1422\n",
      " belles_lettres       0.64      0.05      0.08      1186\n",
      "      adventure       0.83      0.09      0.17      1387\n",
      "        romance       1.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.45      0.13      0.20      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.26     17202\n",
      "      macro avg       0.56      0.15      0.12     17202\n",
      "   weighted avg       0.58      0.26      0.18     17202\n",
      "\n",
      "('Temps_execution(secondes)', 0.8910059928894043)\n",
      "  Logistic Regression classifier : 0.3951\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.40      0.42      0.41      1449\n",
      "        mystery       0.29      0.63      0.39      2121\n",
      "     government       0.70      0.04      0.08       904\n",
      "        hobbies       0.38      0.28      0.32      1268\n",
      "           news       0.87      0.34      0.49       890\n",
      "        fiction       0.62      0.38      0.47      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.38      0.83      0.52      2288\n",
      "      editorial       0.47      0.17      0.25      1422\n",
      " belles_lettres       0.49      0.28      0.35      1186\n",
      "      adventure       0.51      0.46      0.48      1387\n",
      "        romance       0.86      0.06      0.11       515\n",
      "       religion       1.00      0.01      0.01       550\n",
      "science_fiction       0.35      0.40      0.38      1361\n",
      "          humor       1.00      0.01      0.01       266\n",
      "\n",
      "       accuracy                           0.40     17202\n",
      "      macro avg       0.56      0.29      0.29     17202\n",
      "   weighted avg       0.49      0.40      0.36     17202\n",
      "\n",
      "('Temps_execution(secondes)', 162.30290412902832)\n",
      "  linear_svc classifier : 0.5090\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.48      0.50      0.49      1449\n",
      "        mystery       0.43      0.62      0.50      2121\n",
      "     government       0.54      0.23      0.32       904\n",
      "        hobbies       0.43      0.40      0.42      1268\n",
      "           news       0.70      0.59      0.64       890\n",
      "        fiction       0.64      0.60      0.62      1281\n",
      "        reviews       0.69      0.14      0.23       314\n",
      "           lore       0.55      0.80      0.65      2288\n",
      "      editorial       0.50      0.39      0.44      1422\n",
      " belles_lettres       0.45      0.43      0.44      1186\n",
      "      adventure       0.57      0.58      0.58      1387\n",
      "        romance       0.67      0.32      0.43       515\n",
      "       religion       0.73      0.23      0.35       550\n",
      "science_fiction       0.41      0.45      0.43      1361\n",
      "          humor       0.64      0.22      0.33       266\n",
      "\n",
      "       accuracy                           0.51     17202\n",
      "      macro avg       0.56      0.43      0.46     17202\n",
      "   weighted avg       0.53      0.51      0.50     17202\n",
      "\n",
      "('Temps_execution(secondes)', 7.40723729133606)\n",
      "  Random Forest classifier : 0.1345\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.00      0.00      0.00      1449\n",
      "        mystery       0.87      0.01      0.02      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.00      0.00      0.00      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.13      1.00      0.24      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.00      0.00      0.00      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.13     17202\n",
      "      macro avg       0.07      0.07      0.02     17202\n",
      "   weighted avg       0.12      0.13      0.03     17202\n",
      "\n",
      "('Temps_execution(secondes)', 20.93808674812317)\n",
      "  Decision Tree classifier : 0.1635\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.12      0.69      0.20      1449\n",
      "        mystery       0.19      0.07      0.10      2121\n",
      "     government       0.00      0.00      0.00       904\n",
      "        hobbies       0.00      0.00      0.00      1268\n",
      "           news       0.00      0.00      0.00       890\n",
      "        fiction       0.27      0.05      0.09      1281\n",
      "        reviews       0.00      0.00      0.00       314\n",
      "           lore       0.21      0.65      0.32      2288\n",
      "      editorial       0.00      0.00      0.00      1422\n",
      " belles_lettres       0.00      0.00      0.00      1186\n",
      "      adventure       0.00      0.00      0.00      1387\n",
      "        romance       0.00      0.00      0.00       515\n",
      "       religion       0.00      0.00      0.00       550\n",
      "science_fiction       0.18      0.08      0.11      1361\n",
      "          humor       0.00      0.00      0.00       266\n",
      "\n",
      "       accuracy                           0.16     17202\n",
      "      macro avg       0.06      0.10      0.05     17202\n",
      "   weighted avg       0.10      0.16      0.09     17202\n",
      "\n",
      "('Temps_execution(secondes)', 26.424822092056274)\n",
      "  Perceptron multicouche classifier : 0.3885\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        learned       0.53      0.27      0.35      1449\n",
      "        mystery       0.54      0.28      0.37      2121\n",
      "     government       0.54      0.12      0.19       904\n",
      "        hobbies       0.30      0.31      0.31      1268\n",
      "           news       0.54      0.55      0.55       890\n",
      "        fiction       0.60      0.44      0.51      1281\n",
      "        reviews       0.35      0.07      0.12       314\n",
      "           lore       0.50      0.71      0.59      2288\n",
      "      editorial       0.43      0.27      0.33      1422\n",
      " belles_lettres       0.28      0.44      0.34      1186\n",
      "      adventure       0.48      0.44      0.46      1387\n",
      "        romance       0.15      0.49      0.23       515\n",
      "       religion       0.55      0.08      0.13       550\n",
      "science_fiction       0.23      0.49      0.32      1361\n",
      "          humor       0.32      0.12      0.17       266\n",
      "\n",
      "       accuracy                           0.39     17202\n",
      "      macro avg       0.42      0.34      0.33     17202\n",
      "   weighted avg       0.45      0.39      0.38     17202\n",
      "\n",
      "('Temps_execution(secondes)', 849.1698248386383)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase=True,max_df = 0.7)\n",
    "    X = V.fit_transform(brown['tokenized_text'])\n",
    "    y = brown['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(brown['label'])\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5264504127427043, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, True, True]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, True, True]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, True, True]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.16346936402743867, \"['Decision Tree', 1, 3, True, True]\"]\n",
      "[0.1595744680851064, \"['Decision Tree', 1, 2, True, True]\"]\n",
      "[0.15951633530984768, \"['Decision Tree', 1, 1, True, True]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, True, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5264504127427043, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.5264504127427043, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.5264504127427043, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.5264504127427043, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.5227299151261481, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.509010580165097, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.47942099755842343, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.4658179281478898, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.45733054296012093, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.44884315777235206, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, True, True]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, True, False]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, False, True]\"]\n",
      "[0.44082083478665274, \"['Perceptron multicouche', 1, 1, False, False]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.4337867689803511, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.395070340658063, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, True, True]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, True, False]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, False, True]\"]\n",
      "[0.38850133705383094, \"['Perceptron multicouche', 1, 3, False, False]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, True, True]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, True, False]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, False, True]\"]\n",
      "[0.3855365655156377, \"['Perceptron multicouche', 1, 2, False, False]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.35984187885129637, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.285257528194396, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.25863271712591557, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.16439948843157773, \"['Decision Tree', 1, 1, False, False]\"]\n",
      "[0.16346936402743867, \"['Decision Tree', 1, 3, True, True]\"]\n",
      "[0.1627136379490757, \"['Decision Tree', 1, 1, True, False]\"]\n",
      "[0.1595744680851064, \"['Decision Tree', 1, 2, True, True]\"]\n",
      "[0.15951633530984768, \"['Decision Tree', 1, 1, True, True]\"]\n",
      "[0.15759795372631089, \"['Decision Tree', 1, 2, True, False]\"]\n",
      "[0.15533077549122196, \"['Decision Tree', 1, 3, True, False]\"]\n",
      "[0.1519009417509592, \"['Decision Tree', 1, 2, False, True]\"]\n",
      "[0.15050575514475062, \"['Decision Tree', 1, 3, False, True]\"]\n",
      "[0.15021509126845717, \"['Decision Tree', 1, 3, False, False]\"]\n",
      "[0.14661085920241831, \"['Decision Tree', 1, 2, False, False]\"]\n",
      "[0.14562260202302058, \"['Decision Tree', 1, 1, False, True]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.13951866062085805, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.134635507499128, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, True, True]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, False, True]\"]\n",
      "[0.13451924194861062, \"['Random Forest', 1, 3, False, False]\"]\n"
     ]
    }
   ],
   "source": [
    "for res in sorted(Result,reverse=True):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05479ac554d4c29bcd4b23e0ab8fee438ac98ff5737b36f366b93767980d4b08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
