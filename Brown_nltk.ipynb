{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/shailynnxie/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redistribution des neuf catégories en trois grandes catégories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = {}\n",
    "themes[\"news\"] = ['editorial','reviews','news']\n",
    "themes[\"books\"] = [\"adventure\",\"mystery\",\"fiction\",\"romance\"]\n",
    "themes[\"sciences\"] = [\"learned\",\"government\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les statitstiques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news :\n",
      "88 documents\n",
      "202862 mots\n",
      "books :\n",
      "111 documents\n",
      "265021 mots\n",
      "sciences :\n",
      "110 documents\n",
      "252005 mots\n",
      "Textes totals:  309\n"
     ]
    }
   ],
   "source": [
    "nb=0\n",
    "label=[]\n",
    "texte=[]\n",
    "for category in themes.keys():\n",
    "    print(category,\":\")\n",
    "    print(len(brown.fileids(categories=themes[category])),\"documents\")\n",
    "    print(len(brown.words(categories=themes[category])),\"mots\")\n",
    "\n",
    "    nb+=len(brown.fileids(categories=themes[category]))\n",
    "    for cat in themes[category]:\n",
    "        for file in brown.fileids(categories=cat):\n",
    "            label.append(category)\n",
    "            texte.append(' '.join([i for i in brown.words(file)]))\n",
    "print(\"Textes totals: \",nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur : \n",
      "ngram_range (1, 1) :\n",
      "237879\n",
      "ngram_range (1, 2) :\n",
      "758404\n",
      "ngram_range (1, 3) :\n",
      "1347775\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur : \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(texte).getnnz())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur de verteur sans stopwords: \n",
      "ngram_range (1, 1) :\n",
      "197294\n",
      "ngram_range (1, 2) :\n",
      "504898\n",
      "ngram_range (1, 3) :\n",
      "827474\n"
     ]
    }
   ],
   "source": [
    "print(\"longueur de verteur sans stopwords: \")\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False,stop_words='english')\n",
    "    print(\"ngram_range\",(min_N, max_N),\":\")\n",
    "    print(V.fit_transform(texte).getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "liste_classifieurs= [\n",
    "    [\"Perceptron\", Perceptron(eta0=0.1, random_state=0)],\n",
    "    [\"MultinomialNB\", MultinomialNB()],\n",
    "    [\"Logistic Regression\", LogisticRegression()],\n",
    "    [\"linear_svc\", LinearSVC()],\n",
    "    [\"Random Forest\",RandomForestClassifier(n_estimators=250,max_depth=4, random_state=0)],\n",
    "    [\"Decision Tree\", DecisionTreeClassifier(criterion='gini',max_depth=4,splitter='random',min_samples_split=5)],\n",
    "    [\"Perceptron multicouche\",MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(30,20), random_state=1)]\n",
    "    \n",
    "]\n",
    "## Pour éviter les warnings:\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultat=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "chemin_expes = \"brown_nltk_original.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.9355\n",
      "('Temps_execution(secondes)', 0.04068112373352051)\n",
      "  MultinomialNB classifier : 0.7204\n",
      "('Temps_execution(secondes)', 0.005873918533325195)\n",
      "  Logistic Regression classifier : 0.9570\n",
      "('Temps_execution(secondes)', 0.45375680923461914)\n",
      "  linear_svc classifier : 0.9570\n",
      "('Temps_execution(secondes)', 0.028682947158813477)\n",
      "  Random Forest classifier : 0.8925\n",
      "('Temps_execution(secondes)', 0.3713672161102295)\n",
      "  Decision Tree classifier : 0.6667\n",
      "('Temps_execution(secondes)', 0.04468798637390137)\n",
      "  Perceptron multicouche classifier : 0.8602\n",
      "('Temps_execution(secondes)', 5.28367018699646)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.9355\n",
      "('Temps_execution(secondes)', 0.03842496871948242)\n",
      "  MultinomialNB classifier : 0.6882\n",
      "('Temps_execution(secondes)', 0.02271103858947754)\n",
      "  Logistic Regression classifier : 0.8495\n",
      "('Temps_execution(secondes)', 2.3470940589904785)\n",
      "  linear_svc classifier : 0.9462\n",
      "('Temps_execution(secondes)', 0.08297395706176758)\n",
      "  Random Forest classifier : 0.8172\n",
      "('Temps_execution(secondes)', 0.7302930355072021)\n",
      "  Decision Tree classifier : 0.7204\n",
      "('Temps_execution(secondes)', 0.33463215827941895)\n",
      "  Perceptron multicouche classifier : 0.8065\n",
      "('Temps_execution(secondes)', 26.874774932861328)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.9140\n",
      "('Temps_execution(secondes)', 0.0826261043548584)\n",
      "  MultinomialNB classifier : 0.6882\n",
      "('Temps_execution(secondes)', 0.0732579231262207)\n",
      "  Logistic Regression classifier : 0.8065\n",
      "('Temps_execution(secondes)', 5.614201784133911)\n",
      "  linear_svc classifier : 0.9355\n",
      "('Temps_execution(secondes)', 0.15377593040466309)\n",
      "  Random Forest classifier : 0.8495\n",
      "('Temps_execution(secondes)', 1.279839038848877)\n",
      "  Decision Tree classifier : 0.7742\n",
      "('Temps_execution(secondes)', 1.0943968296051025)\n",
      "  Perceptron multicouche classifier : 0.9032\n",
      "('Temps_execution(secondes)', 139.54829692840576)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    X = V.fit_transform(texte)\n",
    "    y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            \n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(label)\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(rep ort)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.956989247311828, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.956989247311828, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.946236559139785, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.9354838709677419, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.9032258064516129, \"['Perceptron multicouche', 1, 3, False, False]\"]\n",
      "[0.8924731182795699, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.8602150537634409, \"['Perceptron multicouche', 1, 1, False, False]\"]\n",
      "[0.8494623655913979, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.8494623655913979, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8172043010752689, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.8064516129032258, \"['Perceptron multicouche', 1, 2, False, False]\"]\n",
      "[0.8064516129032258, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.7741935483870968, \"['Decision Tree', 1, 3, False, False]\"]\n",
      "[0.7204301075268817, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7204301075268817, \"['Decision Tree', 1, 2, False, False]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.6666666666666666, \"['Decision Tree', 1, 1, False, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "Perceptron 1.0\n",
      "MultinomialNB 0.9027777777777778\n",
      "Logistic Regression 0.9953703703703703\n",
      "linear_svc 1.0\n",
      "Random Forest 0.9907407407407407\n",
      "Decision Tree 0.9120370370370371\n",
      "Perceptron multicouche 1.0\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "Perceptron 1.0\n",
      "MultinomialNB 0.9675925925925926\n",
      "Logistic Regression 1.0\n",
      "linear_svc 1.0\n",
      "Random Forest 0.9768518518518519\n",
      "Decision Tree 0.9166666666666666\n",
      "Perceptron multicouche 1.0\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "Perceptron 1.0\n",
      "MultinomialNB 0.9814814814814815\n",
      "Logistic Regression 1.0\n",
      "linear_svc 1.0\n",
      "Random Forest 0.9768518518518519\n",
      "Decision Tree 0.9305555555555556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shailynnxie/Documents/Methodologie/projetFinal/Brown_nltk.ipynb 儲存格 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/Brown_nltk.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNgram_range : (\u001b[39m\u001b[39m{\u001b[39;00mmin_N\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmax_N\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/Brown_nltk.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m nom, algo \u001b[39min\u001b[39;00m liste_classifieurs:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/Brown_nltk.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     clf \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/Brown_nltk.ipynb#X45sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     score \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shailynnxie/Documents/Methodologie/projetFinal/Brown_nltk.ipynb#X45sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(nom,score)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/envi/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:762\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m    746\u001b[0m     \u001b[39m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \n\u001b[1;32m    748\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[39m        Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, incremental\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/envi/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:441\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m# Run the LBFGS solver\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_lbfgs(\n\u001b[1;32m    442\u001b[0m         X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[39m# validate parameter weights\u001b[39;00m\n\u001b[1;32m    446\u001b[0m weights \u001b[39m=\u001b[39m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoefs_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercepts_)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/envi/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m     iprint \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 546\u001b[0m opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m    547\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_grad_lbfgs,\n\u001b[1;32m    548\u001b[0m     packed_coef_inter,\n\u001b[1;32m    549\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    550\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    551\u001b[0m     options\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    552\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxfun\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_fun,\n\u001b[1;32m    553\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    554\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint,\n\u001b[1;32m    555\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    556\u001b[0m     },\n\u001b[1;32m    557\u001b[0m     args\u001b[39m=\u001b[39;49m(X, y, activations, deltas, coef_grads, intercept_grads),\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter)\n\u001b[1;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_ \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/envi/lib/python3.10/site-packages/scipy/optimize/_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    697\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/envi/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:353\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    349\u001b[0m n_iterations \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    351\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[1;32m    352\u001b[0m     \u001b[39m# x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \\\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     _lbfgsb\u001b[39m.\u001b[39;49msetulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n\u001b[1;32m    354\u001b[0m                    pgtol, wa, iwa, task, iprint, csave, lsave,\n\u001b[1;32m    355\u001b[0m                    isave, dsave, maxls)\n\u001b[1;32m    356\u001b[0m     task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    357\u001b[0m     \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    358\u001b[0m         \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m         \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    360\u001b[0m         \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m         \u001b[39m# Overwrite f and g:\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False)\n",
    "    X = V.fit_transform(texte)\n",
    "    y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        clf = algo.fit(X_train, y_train)\n",
    "        score = clf.score(X_train, y_train)\n",
    "       \n",
    "        print(nom,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 0\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"brown_nltk_lower.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.9140\n",
      "('Temps_execution(secondes)', 0.014909029006958008)\n",
      "  MultinomialNB classifier : 0.7312\n",
      "('Temps_execution(secondes)', 0.008095026016235352)\n",
      "  Logistic Regression classifier : 0.9570\n",
      "('Temps_execution(secondes)', 0.3043060302734375)\n",
      "  linear_svc classifier : 0.9570\n",
      "('Temps_execution(secondes)', 0.024254798889160156)\n",
      "  Random Forest classifier : 0.9247\n",
      "('Temps_execution(secondes)', 0.35959911346435547)\n",
      "  Decision Tree classifier : 0.8172\n",
      "('Temps_execution(secondes)', 0.04573512077331543)\n",
      "  Perceptron multicouche classifier : 0.8925\n",
      "('Temps_execution(secondes)', 6.56820821762085)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.9570\n",
      "('Temps_execution(secondes)', 0.0323491096496582)\n",
      "  MultinomialNB classifier : 0.6882\n",
      "('Temps_execution(secondes)', 0.02165818214416504)\n",
      "  Logistic Regression classifier : 0.8817\n",
      "('Temps_execution(secondes)', 2.686736822128296)\n",
      "  linear_svc classifier : 0.9462\n",
      "('Temps_execution(secondes)', 0.07047581672668457)\n",
      "  Random Forest classifier : 0.8387\n",
      "('Temps_execution(secondes)', 0.6233258247375488)\n",
      "  Decision Tree classifier : 0.7312\n",
      "('Temps_execution(secondes)', 0.28900790214538574)\n",
      "  Perceptron multicouche classifier : 0.8602\n",
      "('Temps_execution(secondes)', 22.342143058776855)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.9140\n",
      "('Temps_execution(secondes)', 0.08764219284057617)\n",
      "  MultinomialNB classifier : 0.6882\n",
      "('Temps_execution(secondes)', 0.0670461654663086)\n",
      "  Logistic Regression classifier : 0.8172\n",
      "('Temps_execution(secondes)', 5.940352201461792)\n",
      "  linear_svc classifier : 0.9355\n",
      "('Temps_execution(secondes)', 0.14587974548339844)\n",
      "  Random Forest classifier : 0.8602\n",
      "('Temps_execution(secondes)', 1.2098331451416016)\n",
      "  Decision Tree classifier : 0.7419\n",
      "('Temps_execution(secondes)', 0.8625202178955078)\n",
      "  Perceptron multicouche classifier : 0.9140\n",
      "('Temps_execution(secondes)', 109.91254305839539)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = True)\n",
    "    X = V.fit_transform(texte)\n",
    "    y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            \n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(label)\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(dic_expes, indent=4) # 缩进4字符\n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.956989247311828, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.956989247311828, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.956989247311828, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.946236559139785, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.9354838709677419, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.9247311827956989, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.9139784946236559, \"['Perceptron multicouche', 1, 3, False, True]\"]\n",
      "[0.8924731182795699, \"['Perceptron multicouche', 1, 1, False, True]\"]\n",
      "[0.8817204301075269, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.8602150537634409, \"['Random Forest', 1, 3, False, True]\"]\n",
      "[0.8602150537634409, \"['Perceptron multicouche', 1, 2, False, True]\"]\n",
      "[0.8387096774193549, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.8172043010752689, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.8172043010752689, \"['Decision Tree', 1, 1, False, True]\"]\n",
      "[0.7419354838709677, \"['Decision Tree', 1, 3, False, True]\"]\n",
      "[0.7311827956989247, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.7311827956989247, \"['Decision Tree', 1, 2, False, True]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 2, False, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 0\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"brown_nltk_sw.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.9247\n",
      "('Temps_execution(secondes)', 0.011800050735473633)\n",
      "  MultinomialNB classifier : 0.9032\n",
      "('Temps_execution(secondes)', 0.006679058074951172)\n",
      "  Logistic Regression classifier : 0.9140\n",
      "('Temps_execution(secondes)', 0.2810781002044678)\n",
      "  linear_svc classifier : 0.9677\n",
      "('Temps_execution(secondes)', 0.017364978790283203)\n",
      "  Random Forest classifier : 0.8602\n",
      "('Temps_execution(secondes)', 0.31847333908081055)\n",
      "  Decision Tree classifier : 0.7742\n",
      "('Temps_execution(secondes)', 0.04090714454650879)\n",
      "  Perceptron multicouche classifier : 0.9247\n",
      "('Temps_execution(secondes)', 2.6827070713043213)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.9140\n",
      "('Temps_execution(secondes)', 0.025985240936279297)\n",
      "  MultinomialNB classifier : 0.8172\n",
      "('Temps_execution(secondes)', 0.028022050857543945)\n",
      "  Logistic Regression classifier : 0.8710\n",
      "('Temps_execution(secondes)', 2.4170849323272705)\n",
      "  linear_svc classifier : 0.9247\n",
      "('Temps_execution(secondes)', 0.041166067123413086)\n",
      "  Random Forest classifier : 0.8495\n",
      "('Temps_execution(secondes)', 0.5212218761444092)\n",
      "  Decision Tree classifier : 0.6774\n",
      "('Temps_execution(secondes)', 0.2469499111175537)\n",
      "  Perceptron multicouche classifier : 0.7312\n",
      "('Temps_execution(secondes)', 24.80591583251953)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.9140\n",
      "('Temps_execution(secondes)', 0.04857921600341797)\n",
      "  MultinomialNB classifier : 0.7312\n",
      "('Temps_execution(secondes)', 0.04717421531677246)\n",
      "  Logistic Regression classifier : 0.7957\n",
      "('Temps_execution(secondes)', 4.606475114822388)\n",
      "  linear_svc classifier : 0.8925\n",
      "('Temps_execution(secondes)', 0.07471823692321777)\n",
      "  Random Forest classifier : 0.8065\n",
      "('Temps_execution(secondes)', 0.8589270114898682)\n",
      "  Decision Tree classifier : 0.7634\n",
      "('Temps_execution(secondes)', 0.5212900638580322)\n",
      "  Perceptron multicouche classifier : 0.7957\n",
      "('Temps_execution(secondes)', 47.59680104255676)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = False,stop_words='english')\n",
    "    X = V.fit_transform(texte)\n",
    "    y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            \n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(label)\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(dic_expes, indent=4) # 缩进4字符\n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.967741935483871, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.9247311827956989, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.9247311827956989, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.9247311827956989, \"['Perceptron multicouche', 1, 1, True, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.9139784946236559, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.9032258064516129, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.8924731182795699, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8709677419354839, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8602150537634409, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.8494623655913979, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.8172043010752689, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.8064516129032258, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.7956989247311828, \"['Perceptron multicouche', 1, 3, True, False]\"]\n",
      "[0.7956989247311828, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.7741935483870968, \"['Decision Tree', 1, 1, True, False]\"]\n",
      "[0.7634408602150538, \"['Decision Tree', 1, 3, True, False]\"]\n",
      "[0.7311827956989247, \"['Perceptron multicouche', 1, 2, True, False]\"]\n",
      "[0.7311827956989247, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.6774193548387096, \"['Decision Tree', 1, 2, True, False]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 0\n"
     ]
    }
   ],
   "source": [
    "chemin_expes = \"brown_nltk_sw_lower.json\"\n",
    "\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Perceptron classifier : 0.9355\n",
      "('Temps_execution(secondes)', 0.008861064910888672)\n",
      "  MultinomialNB classifier : 0.9032\n",
      "('Temps_execution(secondes)', 0.0054950714111328125)\n",
      "  Logistic Regression classifier : 0.9032\n",
      "('Temps_execution(secondes)', 0.25968098640441895)\n",
      "  linear_svc classifier : 0.9462\n",
      "('Temps_execution(secondes)', 0.016668081283569336)\n",
      "  Random Forest classifier : 0.8925\n",
      "('Temps_execution(secondes)', 0.3201448917388916)\n",
      "  Decision Tree classifier : 0.7742\n",
      "('Temps_execution(secondes)', 0.041977882385253906)\n",
      "  Perceptron multicouche classifier : 0.9032\n",
      "('Temps_execution(secondes)', 2.3408148288726807)\n",
      "---------------\n",
      "Ngram_range : (1, 2)\n",
      "  Perceptron classifier : 0.9355\n",
      "('Temps_execution(secondes)', 0.025454998016357422)\n",
      "  MultinomialNB classifier : 0.8495\n",
      "('Temps_execution(secondes)', 0.01746511459350586)\n",
      "  Logistic Regression classifier : 0.8602\n",
      "('Temps_execution(secondes)', 2.2236077785491943)\n",
      "  linear_svc classifier : 0.9247\n",
      "('Temps_execution(secondes)', 0.03811287879943848)\n",
      "  Random Forest classifier : 0.8495\n",
      "('Temps_execution(secondes)', 0.5049037933349609)\n",
      "  Decision Tree classifier : 0.7634\n",
      "('Temps_execution(secondes)', 0.2184298038482666)\n",
      "  Perceptron multicouche classifier : 0.7312\n",
      "('Temps_execution(secondes)', 20.38705015182495)\n",
      "---------------\n",
      "Ngram_range : (1, 3)\n",
      "  Perceptron classifier : 0.9032\n",
      "('Temps_execution(secondes)', 0.04483199119567871)\n",
      "  MultinomialNB classifier : 0.7419\n",
      "('Temps_execution(secondes)', 0.03982686996459961)\n",
      "  Logistic Regression classifier : 0.7849\n",
      "('Temps_execution(secondes)', 3.921189308166504)\n",
      "  linear_svc classifier : 0.9032\n",
      "('Temps_execution(secondes)', 0.0661780834197998)\n",
      "  Random Forest classifier : 0.7742\n",
      "('Temps_execution(secondes)', 0.7469019889831543)\n",
      "  Decision Tree classifier : 0.7634\n",
      "('Temps_execution(secondes)', 0.5250060558319092)\n",
      "  Perceptron multicouche classifier : 0.7849\n",
      "('Temps_execution(secondes)', 48.23291897773743)\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = True, True\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = TfidfVectorizer(ngram_range = (min_N, max_N),lowercase = True,stop_words='english')\n",
    "    X = V.fit_transform(texte)\n",
    "    y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            score = dic_expes[expe]\n",
    "            print(expe, \"\\n\",score[0],\"\\n\",score[1],\"\\n\",score[2])\n",
    "            \n",
    "        else:\n",
    "            T1=time.time()\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = [score]\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)\n",
    "            nom_classes=set(label)\n",
    "            report = classification_report(y_test,pred,target_names=nom_classes)\n",
    "            T2=time.time()\n",
    "            calcul_time = (\"Temps_execution(secondes)\",T2 - T1)\n",
    "            dic_expes[expe].append(report)\n",
    "            dic_expes[expe].append(calcul_time)\n",
    "            #print(report)\n",
    "            print(calcul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(dic_expes, indent=4) # 缩进4字符\n",
    "with open(chemin_expes, 'a') as json_file:\n",
    "\tjson_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.946236559139785, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.9247311827956989, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.9032258064516129, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.9032258064516129, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.9032258064516129, \"['Perceptron multicouche', 1, 1, True, True]\"]\n",
      "[0.9032258064516129, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.9032258064516129, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.8924731182795699, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.8602150537634409, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.8494623655913979, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.8494623655913979, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.7849462365591398, \"['Perceptron multicouche', 1, 3, True, True]\"]\n",
      "[0.7849462365591398, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.7741935483870968, \"['Random Forest', 1, 3, True, True]\"]\n",
      "[0.7741935483870968, \"['Decision Tree', 1, 1, True, True]\"]\n",
      "[0.7634408602150538, \"['Decision Tree', 1, 3, True, True]\"]\n",
      "[0.7634408602150538, \"['Decision Tree', 1, 2, True, True]\"]\n",
      "[0.7419354838709677, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.7311827956989247, \"['Perceptron multicouche', 1, 2, True, True]\"]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score[0], nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats,reverse=True):\n",
    "    print(res)\n",
    "    Resultat.append(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.967741935483871, \"['linear_svc', 1, 1, True, False]\"]\n",
      "[0.956989247311828, \"['linear_svc', 1, 1, False, True]\"]\n",
      "[0.956989247311828, \"['linear_svc', 1, 1, False, False]\"]\n",
      "[0.956989247311828, \"['Perceptron', 1, 2, False, True]\"]\n",
      "[0.956989247311828, \"['Logistic Regression', 1, 1, False, True]\"]\n",
      "[0.956989247311828, \"['Logistic Regression', 1, 1, False, False]\"]\n",
      "[0.946236559139785, \"['linear_svc', 1, 2, False, True]\"]\n",
      "[0.946236559139785, \"['linear_svc', 1, 2, False, False]\"]\n",
      "[0.946236559139785, \"['linear_svc', 1, 1, True, True]\"]\n",
      "[0.9354838709677419, \"['linear_svc', 1, 3, False, True]\"]\n",
      "[0.9354838709677419, \"['linear_svc', 1, 3, False, False]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 2, True, True]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 2, False, False]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 1, True, True]\"]\n",
      "[0.9354838709677419, \"['Perceptron', 1, 1, False, False]\"]\n",
      "[0.9247311827956989, \"['linear_svc', 1, 2, True, True]\"]\n",
      "[0.9247311827956989, \"['linear_svc', 1, 2, True, False]\"]\n",
      "[0.9247311827956989, \"['Random Forest', 1, 1, False, True]\"]\n",
      "[0.9247311827956989, \"['Perceptron', 1, 1, True, False]\"]\n",
      "[0.9247311827956989, \"['Perceptron multicouche', 1, 1, True, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 3, True, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 3, False, True]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 3, False, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 2, True, False]\"]\n",
      "[0.9139784946236559, \"['Perceptron', 1, 1, False, True]\"]\n",
      "[0.9139784946236559, \"['Perceptron multicouche', 1, 3, False, True]\"]\n",
      "[0.9139784946236559, \"['Logistic Regression', 1, 1, True, False]\"]\n",
      "[0.9032258064516129, \"['linear_svc', 1, 3, True, True]\"]\n",
      "[0.9032258064516129, \"['Perceptron', 1, 3, True, True]\"]\n",
      "[0.9032258064516129, \"['Perceptron multicouche', 1, 3, False, False]\"]\n",
      "[0.9032258064516129, \"['Perceptron multicouche', 1, 1, True, True]\"]\n",
      "[0.9032258064516129, \"['MultinomialNB', 1, 1, True, True]\"]\n",
      "[0.9032258064516129, \"['MultinomialNB', 1, 1, True, False]\"]\n",
      "[0.9032258064516129, \"['Logistic Regression', 1, 1, True, True]\"]\n",
      "[0.8924731182795699, \"['linear_svc', 1, 3, True, False]\"]\n",
      "[0.8924731182795699, \"['Random Forest', 1, 1, True, True]\"]\n",
      "[0.8924731182795699, \"['Random Forest', 1, 1, False, False]\"]\n",
      "[0.8924731182795699, \"['Perceptron multicouche', 1, 1, False, True]\"]\n",
      "[0.8817204301075269, \"['Logistic Regression', 1, 2, False, True]\"]\n",
      "[0.8709677419354839, \"['Logistic Regression', 1, 2, True, False]\"]\n",
      "[0.8602150537634409, \"['Random Forest', 1, 3, False, True]\"]\n",
      "[0.8602150537634409, \"['Random Forest', 1, 1, True, False]\"]\n",
      "[0.8602150537634409, \"['Perceptron multicouche', 1, 2, False, True]\"]\n",
      "[0.8602150537634409, \"['Perceptron multicouche', 1, 1, False, False]\"]\n",
      "[0.8602150537634409, \"['Logistic Regression', 1, 2, True, True]\"]\n",
      "[0.8494623655913979, \"['Random Forest', 1, 3, False, False]\"]\n",
      "[0.8494623655913979, \"['Random Forest', 1, 2, True, True]\"]\n",
      "[0.8494623655913979, \"['Random Forest', 1, 2, True, False]\"]\n",
      "[0.8494623655913979, \"['MultinomialNB', 1, 2, True, True]\"]\n",
      "[0.8494623655913979, \"['Logistic Regression', 1, 2, False, False]\"]\n",
      "[0.8387096774193549, \"['Random Forest', 1, 2, False, True]\"]\n",
      "[0.8172043010752689, \"['Random Forest', 1, 2, False, False]\"]\n",
      "[0.8172043010752689, \"['MultinomialNB', 1, 2, True, False]\"]\n",
      "[0.8172043010752689, \"['Logistic Regression', 1, 3, False, True]\"]\n",
      "[0.8172043010752689, \"['Decision Tree', 1, 1, False, True]\"]\n",
      "[0.8064516129032258, \"['Random Forest', 1, 3, True, False]\"]\n",
      "[0.8064516129032258, \"['Perceptron multicouche', 1, 2, False, False]\"]\n",
      "[0.8064516129032258, \"['Logistic Regression', 1, 3, False, False]\"]\n",
      "[0.7956989247311828, \"['Perceptron multicouche', 1, 3, True, False]\"]\n",
      "[0.7956989247311828, \"['Logistic Regression', 1, 3, True, False]\"]\n",
      "[0.7849462365591398, \"['Perceptron multicouche', 1, 3, True, True]\"]\n",
      "[0.7849462365591398, \"['Logistic Regression', 1, 3, True, True]\"]\n",
      "[0.7741935483870968, \"['Random Forest', 1, 3, True, True]\"]\n",
      "[0.7741935483870968, \"['Decision Tree', 1, 3, False, False]\"]\n",
      "[0.7741935483870968, \"['Decision Tree', 1, 1, True, True]\"]\n",
      "[0.7741935483870968, \"['Decision Tree', 1, 1, True, False]\"]\n",
      "[0.7634408602150538, \"['Decision Tree', 1, 3, True, True]\"]\n",
      "[0.7634408602150538, \"['Decision Tree', 1, 3, True, False]\"]\n",
      "[0.7634408602150538, \"['Decision Tree', 1, 2, True, True]\"]\n",
      "[0.7419354838709677, \"['MultinomialNB', 1, 3, True, True]\"]\n",
      "[0.7419354838709677, \"['Decision Tree', 1, 3, False, True]\"]\n",
      "[0.7311827956989247, \"['Perceptron multicouche', 1, 2, True, True]\"]\n",
      "[0.7311827956989247, \"['Perceptron multicouche', 1, 2, True, False]\"]\n",
      "[0.7311827956989247, \"['MultinomialNB', 1, 3, True, False]\"]\n",
      "[0.7311827956989247, \"['MultinomialNB', 1, 1, False, True]\"]\n",
      "[0.7311827956989247, \"['Decision Tree', 1, 2, False, True]\"]\n",
      "[0.7204301075268817, \"['MultinomialNB', 1, 1, False, False]\"]\n",
      "[0.7204301075268817, \"['Decision Tree', 1, 2, False, False]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 3, False, True]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 3, False, False]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 2, False, True]\"]\n",
      "[0.6881720430107527, \"['MultinomialNB', 1, 2, False, False]\"]\n",
      "[0.6774193548387096, \"['Decision Tree', 1, 2, True, False]\"]\n",
      "[0.6666666666666666, \"['Decision Tree', 1, 1, False, False]\"]\n"
     ]
    }
   ],
   "source": [
    "for res in sorted(Resultat,reverse=True):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "envi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
